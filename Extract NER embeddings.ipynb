{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting POS tags (Experimenting)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7a153b5145d5d81"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Italian                    2. French                     3. Spanish; Castilian       \n",
      "  4. Bulgarian                  5. Slovene                    6. Irish                    \n",
      "  7. Finnish                    8. Dutch                      9. Swedish                  \n",
      " 10. Danish                    11. Portuguese                12. English                  \n",
      " 13. German                    14. Indonesian                15. Czech                    \n",
      " 16. Hungarian                \n",
      "[polyglot_data] Error loading pos2.mk: Package 'pos2.mk' not found in\n",
      "[polyglot_data]     index\n"
     ]
    }
   ],
   "source": [
    "import polyglot\n",
    "from polyglot.downloader import downloader\n",
    "\n",
    "print(downloader.supported_languages_table(\"pos2\", 3))\n",
    "downloader.download(\"pos2.mk\")\n",
    "\n",
    "from polyglot.text import Text\n",
    "\n",
    "def get_pos_tags_polyglot(text, lang='mk'):\n",
    "    poly_text = Text(text, hint_language_code=lang)\n",
    "    return ' '.join([f'{word}_{tag}' for word, tag in zip(poly_text.words, poly_text.pos_tags)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T02:09:11.024350Z",
     "start_time": "2024-04-20T02:09:10.072210Z"
    }
   },
   "id": "93470849cdc96b99",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c36d2f3f79b43339b2e6e533186a775"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:16:11 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:16:11 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-04-19 22:16:13 INFO: File exists: /Users/iqbal/stanza_resources/en/default.zip\n",
      "2024-04-19 22:16:15 INFO: Finished downloading models and saved to /Users/iqbal/stanza_resources\n",
      "2024-04-19 22:16:15 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f873893651b84af98ebc663bdb002394"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:16:15 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:16:15 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-04-19 22:16:16 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2024-04-19 22:16:16 INFO: Using device: cpu\n",
      "2024-04-19 22:16:16 INFO: Loading: tokenize\n",
      "2024-04-19 22:16:16 INFO: Loading: mwt\n",
      "2024-04-19 22:16:16 INFO: Loading: pos\n",
      "2024-04-19 22:16:16 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: This_PRON is_AUX a_DET sample_NOUN sentence_NOUN ._PUNCT\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4ca2ef30c77440285f476e67d7b957e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:16:16 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:16:16 INFO: Downloading default packages for language: ar (Arabic) ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://huggingface.co/stanfordnlp/stanza-ar/resolve/v1.8.0/models/default.zip:   0%|          | 0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09b434b240154500a293ca72ab0986ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:16:29 INFO: Downloaded file to /Users/iqbal/stanza_resources/ar/default.zip\n",
      "2024-04-19 22:16:30 INFO: Finished downloading models and saved to /Users/iqbal/stanza_resources\n",
      "2024-04-19 22:16:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88a9e47db20a4a1aba4b80721ff1fa99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:16:31 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:16:31 WARNING: Language ar package default expects mwt, which has been added\n",
      "2024-04-19 22:16:31 INFO: Loading these models for language: ar (Arabic):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | padt        |\n",
      "| mwt       | padt        |\n",
      "| pos       | padt_charlm |\n",
      "===========================\n",
      "\n",
      "2024-04-19 22:16:31 INFO: Using device: cpu\n",
      "2024-04-19 22:16:31 INFO: Loading: tokenize\n",
      "2024-04-19 22:16:31 INFO: Loading: mwt\n",
      "2024-04-19 22:16:31 INFO: Loading: pos\n",
      "2024-04-19 22:16:31 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic: هذا_DET نص_NOUN عربي_ADJ ل_ADP التحليل_NOUN ._PUNCT\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5698e1ae3cd146298fd92de18b8f4d09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:16:32 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:16:32 INFO: Downloading default packages for language: bg (Bulgarian) ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://huggingface.co/stanfordnlp/stanza-bg/resolve/v1.8.0/models/default.zip:   0%|          | 0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dfb3c81f04c43ada84dfbf9885d19ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:16:41 INFO: Downloaded file to /Users/iqbal/stanza_resources/bg/default.zip\n",
      "2024-04-19 22:16:42 INFO: Finished downloading models and saved to /Users/iqbal/stanza_resources\n",
      "2024-04-19 22:16:42 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "582fdf101fcb4b0bad4d8ecd358f36f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:16:42 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:16:43 INFO: Loading these models for language: bg (Bulgarian):\n",
      "==========================\n",
      "| Processor | Package    |\n",
      "--------------------------\n",
      "| tokenize  | btb        |\n",
      "| pos       | btb_charlm |\n",
      "==========================\n",
      "\n",
      "2024-04-19 22:16:43 INFO: Using device: cpu\n",
      "2024-04-19 22:16:43 INFO: Loading: tokenize\n",
      "2024-04-19 22:16:43 INFO: Loading: pos\n",
      "2024-04-19 22:16:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulgarian: Това_PRON е_AUX примерен_ADJ текст_NOUN за_ADP анализ_NOUN ._PUNCT\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Dictionary to hold the Stanza pipeline objects for each language\n",
    "nlp_pipelines = {}\n",
    "\n",
    "def get_stanza_pipeline(lang):\n",
    "    if lang not in nlp_pipelines:\n",
    "        # Download the model if not already downloaded\n",
    "        stanza.download(lang)\n",
    "        # Create a Stanza pipeline for the specified language\n",
    "        nlp_pipelines[lang] = stanza.Pipeline(lang, processors='tokenize,pos')\n",
    "    return nlp_pipelines[lang]\n",
    "\n",
    "def get_pos_tags_stanza(text, lang='en'):\n",
    "    # Retrieve the appropriate NLP pipeline based on language\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    # Generate POS tags for each token in the text\n",
    "    \n",
    "    pos_tags = ' '.join([f'{word.text}_{word.upos}' for sent in doc.sentences for word in sent.words])\n",
    "    return pos_tags\n",
    "\n",
    "# Example usage\n",
    "english_text = \"This is a sample sentence.\"\n",
    "print(\"English:\", get_pos_tags_stanza(english_text, 'en'))\n",
    "\n",
    "arabic_text = \"هذا نص عربي للتحليل.\"\n",
    "print(\"Arabic:\", get_pos_tags_stanza(arabic_text, 'ar'))\n",
    "\n",
    "bulgarian_text = \"Това е примерен текст за анализ.\"\n",
    "print(\"Bulgarian:\", get_pos_tags_stanza(bulgarian_text, 'bg'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T02:16:43.490437Z",
     "start_time": "2024-04-20T02:16:11.578840Z"
    }
   },
   "id": "5653ed7ab21c4616",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 114\u001B[0m\n\u001B[1;32m    110\u001B[0m test_bg_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./test_labels_ar_bg_md_version2/POS/test_subtask1_bg.json\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m# print(json.dumps(updated_data, indent=4))\u001B[39;00m\n\u001B[0;32m--> 114\u001B[0m updated_data \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m updated_data \u001B[38;5;241m=\u001B[39m process_data(val_input, val_output, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    116\u001B[0m updated_data \u001B[38;5;241m=\u001B[39m process_data(test_en_input, test_en_output, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[7], line 80\u001B[0m, in \u001B[0;36mprocess_data\u001B[0;34m(filename, output_filename, lang)\u001B[0m\n\u001B[1;32m     78\u001B[0m preprocessed_text \u001B[38;5;241m=\u001B[39m preprocess_text(item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     79\u001B[0m \u001B[38;5;66;03m# Get POS tags\u001B[39;00m\n\u001B[0;32m---> 80\u001B[0m pos_tags \u001B[38;5;241m=\u001B[39m \u001B[43mget_pos_tags\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocessed_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# Update the item with POS tags\u001B[39;00m\n\u001B[1;32m     82\u001B[0m item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pos_tags\n",
      "Cell \u001B[0;32mIn[7], line 66\u001B[0m, in \u001B[0;36mget_pos_tags\u001B[0;34m(text, lang)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_pos_tags\u001B[39m(text, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m lang \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mar\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbg\u001B[39m\u001B[38;5;124m'\u001B[39m]:  \u001B[38;5;66;03m# supported by Stanza\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_pos_tags_stanza\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m lang \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmk\u001B[39m\u001B[38;5;124m'\u001B[39m:  \u001B[38;5;66;03m# use Polyglot for Macedonian\u001B[39;00m\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m get_pos_tags_polyglot(text, lang)\n",
      "Cell \u001B[0;32mIn[6], line 17\u001B[0m, in \u001B[0;36mget_pos_tags_stanza\u001B[0;34m(text, lang)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_pos_tags_stanza\u001B[39m(text, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# Retrieve the appropriate NLP pipeline based on language\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     nlp \u001B[38;5;241m=\u001B[39m get_stanza_pipeline(lang)\n\u001B[0;32m---> 17\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# Generate POS tags for each token in the text\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     pos_tags \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mword\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mword\u001B[38;5;241m.\u001B[39mupos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m doc\u001B[38;5;241m.\u001B[39msentences \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m sent\u001B[38;5;241m.\u001B[39mwords])\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/pipeline/core.py:477\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[0;34m(self, doc, processors)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, doc, processors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 477\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/pipeline/core.py:428\u001B[0m, in \u001B[0;36mPipeline.process\u001B[0;34m(self, doc, processors)\u001B[0m\n\u001B[1;32m    426\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors\u001B[38;5;241m.\u001B[39mget(processor_name):\n\u001B[1;32m    427\u001B[0m         process \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors[processor_name]\u001B[38;5;241m.\u001B[39mbulk_process \u001B[38;5;28;01mif\u001B[39;00m bulk \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors[processor_name]\u001B[38;5;241m.\u001B[39mprocess\n\u001B[0;32m--> 428\u001B[0m         doc \u001B[38;5;241m=\u001B[39m \u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/pipeline/pos_processor.py:85\u001B[0m, in \u001B[0;36mPOSProcessor.process\u001B[0;34m(self, document)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(batch):\n\u001B[1;32m     84\u001B[0m             idx\u001B[38;5;241m.\u001B[39mextend(b[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m---> 85\u001B[0m             preds \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     87\u001B[0m preds \u001B[38;5;241m=\u001B[39m unsort(preds, idx)\n\u001B[1;32m     88\u001B[0m dataset\u001B[38;5;241m.\u001B[39mdoc\u001B[38;5;241m.\u001B[39mset([doc\u001B[38;5;241m.\u001B[39mUPOS, doc\u001B[38;5;241m.\u001B[39mXPOS, doc\u001B[38;5;241m.\u001B[39mFEATS], [y \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m preds \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m x])\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/models/pos/trainer.py:87\u001B[0m, in \u001B[0;36mTrainer.predict\u001B[0;34m(self, batch, unsort)\u001B[0m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     86\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m word\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 87\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwordchars\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwordchars_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxpos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mufeats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_orig_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwordlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m upos_seqs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mupos\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munmap(sent) \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m preds[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()]\n\u001B[1;32m     89\u001B[0m xpos_seqs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxpos\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munmap(sent) \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m preds[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()]\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/models/pos/model.py:172\u001B[0m, in \u001B[0;36mTagger.forward\u001B[0;34m(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_emb_dim\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcharlm\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 172\u001B[0m         all_forward_chars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmodel_forward\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_char_representation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(all_forward_chars, \u001B[38;5;28mlist\u001B[39m)\n\u001B[1;32m    174\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcharmodel_forward_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/models/common/char_model.py:222\u001B[0m, in \u001B[0;36mCharacterLanguageModel.build_char_representation\u001B[0;34m(self, sentences)\u001B[0m\n\u001B[1;32m    219\u001B[0m chars \u001B[38;5;241m=\u001B[39m get_long_tensor(chars, \u001B[38;5;28mlen\u001B[39m(all_data), pad_id\u001B[38;5;241m=\u001B[39mvocab\u001B[38;5;241m.\u001B[39munit2id(CHARLM_END))\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 222\u001B[0m     output, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchars\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchar_lens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    223\u001B[0m     res \u001B[38;5;241m=\u001B[39m [output[i, offsets] \u001B[38;5;28;01mfor\u001B[39;00m i, offsets \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(char_offsets)]\n\u001B[1;32m    224\u001B[0m     res \u001B[38;5;241m=\u001B[39m unsort(res, orig_idx)\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/models/common/char_model.py:153\u001B[0m, in \u001B[0;36mCharacterLanguageModel.forward\u001B[0;34m(self, chars, charlens, hidden)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hidden \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: \n\u001B[1;32m    151\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcharlstm_h_init\u001B[38;5;241m.\u001B[39mexpand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_num_layers\u001B[39m\u001B[38;5;124m'\u001B[39m], batch_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_hidden_dim\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mcontiguous(),\n\u001B[1;32m    152\u001B[0m               \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcharlstm_c_init\u001B[38;5;241m.\u001B[39mexpand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_num_layers\u001B[39m\u001B[38;5;124m'\u001B[39m], batch_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_hidden_dim\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mcontiguous())\n\u001B[0;32m--> 153\u001B[0m output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43membs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcharlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pad_packed_sequence(output, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    155\u001B[0m decoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(output)\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/models/common/packed_lstm.py:22\u001B[0m, in \u001B[0;36mPackedLSTM.forward\u001B[0;34m(self, input, lengths, hx)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, PackedSequence):\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m pack_padded_sequence(\u001B[38;5;28minput\u001B[39m, lengths, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n\u001B[0;32m---> 22\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad:\n\u001B[1;32m     24\u001B[0m     res \u001B[38;5;241m=\u001B[39m (pad_packed_sequence(res[\u001B[38;5;241m0\u001B[39m], batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)[\u001B[38;5;241m0\u001B[39m], res[\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:882\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    879\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m    880\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 882\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    884\u001B[0m output \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    885\u001B[0m hidden \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;241m1\u001B[39m:]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load JSON data from a file\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Save updated JSON data to a file\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Decode HTML entities\n",
    "    # text = html.unescape(text)\n",
    "    \n",
    "    # Replace \\\\n and \\n (new lines) with a space\n",
    "    # print(text)\n",
    "    # print()\n",
    "    text = re.sub(r'(\\\\n)+', ' ', text)\n",
    "    \n",
    "    # print(text)\n",
    "    # print()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove any remaining backslashes\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    \n",
    "    # print(text)\n",
    "    # print()\n",
    "    \n",
    "    # Remove digits - You can comment this line if you want to keep numbers\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Optionally, remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', punctuation))\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    # text = text.lower()\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # print(text)\n",
    "    # print()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Get POS tags using spaCy\n",
    "def get_pos_tags(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([f'{token}_{token.pos_}'  for token in doc])\n",
    "    # return [token.pos_ for token in doc]\n",
    "\n",
    "def get_pos_tags(text, lang='en'):\n",
    "    if lang in ['en', 'ar', 'bg']:  # supported by Stanza\n",
    "        return get_pos_tags_stanza(text, lang)\n",
    "    elif lang == 'mk':  # use Polyglot for Macedonian\n",
    "        return get_pos_tags_polyglot(text, lang)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"POS tagging not implemented for language: {lang}\")\n",
    "\n",
    "\n",
    "# Main function to process the data\n",
    "def process_data(filename, output_filename, lang='en'):\n",
    "    data = load_data(filename)\n",
    "    for item in data:\n",
    "        # Preprocess the text\n",
    "        preprocessed_text = preprocess_text(item['text'])\n",
    "        # Get POS tags\n",
    "        pos_tags = get_pos_tags(preprocessed_text, lang)\n",
    "        # Update the item with POS tags\n",
    "        item['text'] = pos_tags\n",
    "        # item['processed_text'] = preprocessed_text\n",
    "        \n",
    "    # Save updated data\n",
    "    save_data(data, output_filename)\n",
    "    return data\n",
    "\n",
    "# Example of usage\n",
    "if __name__ == \"__main__\":\n",
    "    filename = 'semeval2024_dev_release/subtask1/validation.json'\n",
    "    output_file_path = 'semeval2024_dev_release/subtask1/pos_validation.json'\n",
    "    \n",
    "    train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "    train_output = './semeval2024_dev_release/subtask1/POS/train.json'\n",
    "    \n",
    "    val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "    val_output = './semeval2024_dev_release/subtask1/POS/validation.json'\n",
    "    \n",
    "    test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "    test_en_output = './test_data/english/POS/en_subtask1_test_unlabeled.json'\n",
    "    \n",
    "    test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "    test_md_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_md.json'\n",
    "    \n",
    "    test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "    test_ar_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_ar.json'\n",
    "    \n",
    "    test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "    test_bg_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_bg.json'\n",
    "    \n",
    "    # print(json.dumps(updated_data, indent=4))\n",
    "    \n",
    "    updated_data = process_data(train_input, train_output, lang='en')\n",
    "    updated_data = process_data(val_input, val_output, lang='en')\n",
    "    updated_data = process_data(test_en_input, test_en_output, lang='en')\n",
    "    updated_data = process_data(test_bg_input, test_bg_output, lang='bg')\n",
    "    updated_data = process_data(test_ar_input, test_ar_output, lang='ar')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T02:23:32.381905Z",
     "start_time": "2024-04-20T02:19:13.208394Z"
    }
   },
   "id": "6dd872adbb4f2c76",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59a4ba3981a94b36b596a0008d4e0dce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:24:52 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:24:52 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-04-19 22:24:54 INFO: File exists: /Users/iqbal/stanza_resources/en/default.zip\n",
      "2024-04-19 22:24:56 INFO: Finished downloading models and saved to /Users/iqbal/stanza_resources\n",
      "2024-04-19 22:24:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "455a4363c7d7452f921602979fd4ab4e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:24:57 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:24:57 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-04-19 22:24:57 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2024-04-19 22:24:57 WARNING: GPU requested, but is not available!\n",
      "2024-04-19 22:24:57 INFO: Using device: cpu\n",
      "2024-04-19 22:24:57 INFO: Loading: tokenize\n",
      "2024-04-19 22:24:57 INFO: Loading: mwt\n",
      "2024-04-19 22:24:57 INFO: Loading: pos\n",
      "2024-04-19 22:24:57 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: This_PRON is_AUX a_DET sample_NOUN sentence_NOUN ._PUNCT\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e0dfb567c3e4e4ca56e5ed8598e33f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:24:58 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:24:58 INFO: Downloading default packages for language: ar (Arabic) ...\n",
      "2024-04-19 22:24:59 INFO: File exists: /Users/iqbal/stanza_resources/ar/default.zip\n",
      "2024-04-19 22:25:01 INFO: Finished downloading models and saved to /Users/iqbal/stanza_resources\n",
      "2024-04-19 22:25:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "128697b9e54b41a7aea2120de2b80911"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:25:01 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:25:01 WARNING: Language ar package default expects mwt, which has been added\n",
      "2024-04-19 22:25:02 INFO: Loading these models for language: ar (Arabic):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | padt        |\n",
      "| mwt       | padt        |\n",
      "| pos       | padt_charlm |\n",
      "===========================\n",
      "\n",
      "2024-04-19 22:25:02 WARNING: GPU requested, but is not available!\n",
      "2024-04-19 22:25:02 INFO: Using device: cpu\n",
      "2024-04-19 22:25:02 INFO: Loading: tokenize\n",
      "2024-04-19 22:25:02 INFO: Loading: mwt\n",
      "2024-04-19 22:25:02 INFO: Loading: pos\n",
      "2024-04-19 22:25:02 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic: هذا_DET نص_NOUN عربي_ADJ ل_ADP التحليل_NOUN ._PUNCT\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be22b212820d46238966ba2ceeefe649"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:25:02 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:25:02 INFO: Downloading default packages for language: bg (Bulgarian) ...\n",
      "2024-04-19 22:25:03 INFO: File exists: /Users/iqbal/stanza_resources/bg/default.zip\n",
      "2024-04-19 22:25:04 INFO: Finished downloading models and saved to /Users/iqbal/stanza_resources\n",
      "2024-04-19 22:25:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b8d1943485a43cfb4a7ea8d7d613d18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 22:25:05 INFO: Downloaded file to /Users/iqbal/stanza_resources/resources.json\n",
      "2024-04-19 22:25:05 INFO: Loading these models for language: bg (Bulgarian):\n",
      "==========================\n",
      "| Processor | Package    |\n",
      "--------------------------\n",
      "| tokenize  | btb        |\n",
      "| pos       | btb_charlm |\n",
      "==========================\n",
      "\n",
      "2024-04-19 22:25:05 WARNING: GPU requested, but is not available!\n",
      "2024-04-19 22:25:05 INFO: Using device: cpu\n",
      "2024-04-19 22:25:05 INFO: Loading: tokenize\n",
      "2024-04-19 22:25:05 INFO: Loading: pos\n",
      "2024-04-19 22:25:05 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulgarian: Това_PRON е_AUX примерен_ADJ текст_NOUN за_ADP анализ_NOUN ._PUNCT\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "class NLPModelManager:\n",
    "    def __init__(self):\n",
    "        self.nlp_pipelines = {}\n",
    "\n",
    "    def get_pipeline(self, lang='en'):\n",
    "        if lang not in self.nlp_pipelines:\n",
    "            # Download the model if not already downloaded\n",
    "            stanza.download(lang)\n",
    "            # Create a Stanza pipeline for the specified language\n",
    "            self.nlp_pipelines[lang] = stanza.Pipeline(lang, processors='tokenize,pos', use_gpu=True)\n",
    "        return self.nlp_pipelines[lang]\n",
    "\n",
    "# Create a global instance of the NLPModelManager\n",
    "nlp_manager = NLPModelManager()\n",
    "\n",
    "def get_pos_tags_stanza(text, lang='en'):\n",
    "    # Retrieve the appropriate NLP pipeline based on language from the manager\n",
    "    nlp = nlp_manager.get_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    # Generate POS tags for each token in the text\n",
    "    pos_tags = ' '.join([f'{word.text}_{word.upos}' for sent in doc.sentences for word in sent.words])\n",
    "    return pos_tags\n",
    "\n",
    "# Example usage\n",
    "english_text = \"This is a sample sentence.\"\n",
    "print(\"English:\", get_pos_tags_stanza(english_text, 'en'))\n",
    "\n",
    "arabic_text = \"هذا نص عربي للتحليل.\"\n",
    "print(\"Arabic:\", get_pos_tags_stanza(arabic_text, 'ar'))\n",
    "\n",
    "bulgarian_text = \"Това е примерен текст за анализ.\"\n",
    "print(\"Bulgarian:\", get_pos_tags_stanza(bulgarian_text, 'bg'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T02:25:05.988036Z",
     "start_time": "2024-04-20T02:24:52.736457Z"
    }
   },
   "id": "246f7be14882b67b",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_text_item(item, lang='en'):\n",
    "    preprocessed_text = preprocess_text(item['text'])\n",
    "    pos_tags = get_pos_tags(preprocessed_text, lang)\n",
    "    item['text'] = pos_tags\n",
    "    return item\n",
    "\n",
    "def process_data_concurrently(data, lang='en'):\n",
    "    processed_items = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_item = {executor.submit(process_text_item, item, lang): item for item in data}\n",
    "        for future in as_completed(future_to_item):\n",
    "            processed_items.append(future.result())\n",
    "    return processed_items\n",
    "\n",
    "def batch_process(data, batch_size=10, lang='en'):\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        texts = [preprocess_text(item['text']) for item in batch]\n",
    "        docs = nlp(texts)\n",
    "        for doc, item in zip(docs, batch):\n",
    "            pos_tags = ' '.join([f'{word.text}_{word.upos}' for sent in doc.sentences for word in sent.words])\n",
    "            item['text'] = pos_tags\n",
    "    return data\n",
    "\n",
    "# import cProfile\n",
    "# import pstats\n",
    "# \n",
    "# with cProfile.Profile() as pr:\n",
    "#     process_data(test_ar_input, test_ar_output, lang='ar')\n",
    "# \n",
    "# stats = pstats.Stats(pr)\n",
    "# stats.sort_stats(pstats.SortKey.TIME)\n",
    "# stats.print_stats()\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    filename = 'semeval2024_dev_release/subtask1/validation.json'\n",
    "    output_file_path = 'semeval2024_dev_release/subtask1/pos_validation.json'\n",
    "\n",
    "    train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "    train_output = './semeval2024_dev_release/subtask1/POS/train.json'\n",
    "\n",
    "    val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "    val_output = './semeval2024_dev_release/subtask1/POS/validation.json'\n",
    "\n",
    "    test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "    test_en_output = './test_data/english/POS/en_subtask1_test_unlabeled.json'\n",
    "\n",
    "    test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "    test_md_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_md.json'\n",
    "\n",
    "    test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "    test_ar_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_ar.json'\n",
    "\n",
    "    test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "    test_bg_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_bg.json'\n",
    "\n",
    "    # print(json.dumps(updated_data, indent=4))\n",
    "\n",
    "    # updated_data = process_data(train_input, train_output, lang='en')\n",
    "    # updated_data = process_data(val_input, val_output, lang='en')\n",
    "    updated_data = process_data(test_en_input, test_en_output, lang='en')\n",
    "    updated_data = process_data(test_bg_input, test_bg_output, lang='bg')\n",
    "    updated_data = process_data(test_ar_input, test_ar_output, lang='ar')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T04:19:16.375759Z",
     "start_time": "2024-04-20T04:15:52.997770Z"
    }
   },
   "id": "1a041f91fd35ede7",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "da4281cfe2f6d34c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting NER embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ff47d38903a0ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import torchvision.models as models\n",
    "from utils.utils import *\n",
    "from utils.label_decoding import *\n",
    "from utils.HierarchicalLoss import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T00:11:07.653701Z",
     "start_time": "2024-04-29T00:11:07.501048Z"
    }
   },
   "id": "661f00377194cb64",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\", output_hidden_states=True)\n",
    "\n",
    "# Encode the input text and obtain the model outputs\n",
    "text = \"Insert your text here.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "outputs = model(**inputs)\n",
    "hidden_states = outputs.hidden_states\n",
    "token_embeddings = hidden_states[-1][0] \n",
    "\n",
    "\n",
    "# Strategy 2: Use the [CLS] token embedding\n",
    "cls_embedding = token_embeddings[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T00:10:45.089822Z",
     "start_time": "2024-04-29T00:10:43.954620Z"
    }
   },
   "id": "4e216a55eab5b3b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([768])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embedding.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-26T21:52:28.668618Z",
     "start_time": "2024-04-26T21:52:28.665880Z"
    }
   },
   "id": "32cd4b3aa566dc25",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T00:11:11.298246Z",
     "start_time": "2024-04-29T00:11:11.276235Z"
    }
   },
   "id": "4db099ecce6e8648",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def process_json(file_path, techniques_to_level, hierarchy):\n",
    "    data_df = pd.read_json(file_path)\n",
    "    data_df['cleaned_text'] = data_df['text'].apply(replace_newlines_with_fullstop)\n",
    "    if 'link' in data_df.columns:\n",
    "        data_df.drop(columns=['link'], inplace=True)\n",
    "\n",
    "    # for level in ['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5']:\n",
    "    #     data_df[level] = pd.Series([[] for _ in range(len(data_df))], index=data_df.index)\n",
    "    \n",
    "    return data_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T00:11:31.683112Z",
     "start_time": "2024-04-29T00:11:31.680083Z"
    }
   },
   "id": "f86283fdbca15d28",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_text_features(file_path, tokenizer, text_model, output_file_path, subtask=1, device=torch.device(\"mps\")):\n",
    "    features_dict = {}\n",
    "    \n",
    "    if subtask == 1:\n",
    "        data = process_json(file_path, techniques_to_level_1, hierarchy_1)\n",
    "    else:\n",
    "        data = process_json(file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for id, text in zip(data['id'], data['cleaned_text']):\n",
    "        # print(data['text'], data['cleaned_text'])\n",
    "        # break\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True, \n",
    "                                  max_length=128, truncation=True, padding='max_length').to(device)\n",
    "        \n",
    "        # input_ids = encoded_input['input_ids'].to('cpu')\n",
    "        # decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "        # print(decoded_text)\n",
    "        # return\n",
    "        with torch.no_grad():\n",
    "            embeddings = text_model(**encoded_input)\n",
    "        features_dict[id] = embeddings.hidden_states[-1][:, 0, :].detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        # print(features_dict[id].shape, features_dict[id].dtype)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f'completed {step} steps')\n",
    "            \n",
    "    with open(f'{output_file_path}', 'wb') as f:\n",
    "        pickle.dump(features_dict, f)\n",
    "    \n",
    "    print(f\"Features extracted and stored in {output_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T00:11:43.592875Z",
     "start_time": "2024-04-29T00:11:43.588481Z"
    }
   },
   "id": "2ed8634e790038b1",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/BERT-NER/en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\", output_hidden_states=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dir = './TextFeatures/subtask1a/BERT-NER/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "dev_en_input = './semeval2024_dev_release/subtask1/dev_unlabeled.json'\n",
    "dev_en_output = dir + 'en_dev_text_features.pkl'\n",
    "\n",
    "# extract_text_features(train_input, tokenizer, model, train_output)\n",
    "# extract_text_features(val_input, tokenizer, model, val_output)\n",
    "# \n",
    "# extract_text_features(test_en_input, tokenizer, model, test_en_output)\n",
    "# extract_text_features(test_md_input, tokenizer, model, test_md_output)\n",
    "# extract_text_features(test_ar_input, tokenizer, model, test_ar_output)\n",
    "# extract_text_features(test_bg_input, tokenizer, model, test_bg_output)\n",
    "\n",
    "extract_text_features(dev_en_input, tokenizer, model, dev_en_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T00:35:41.791230Z",
     "start_time": "2024-04-29T00:35:17.245244Z"
    }
   },
   "id": "92ffaf21e67d8adc",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-large-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/BERT-NER/en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\",\n",
    "                                                        output_hidden_states=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dir = './TextFeatures/subtask1a/multilingual-ner/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "dev_en_input = './semeval2024_dev_release/subtask1/dev_unlabeled.json'\n",
    "dev_en_output = dir + 'en_dev_text_features.pkl'\n",
    "\n",
    "# extract_text_features(train_input, tokenizer, model, train_output)\n",
    "# extract_text_features(val_input, tokenizer, model, val_output)\n",
    "# \n",
    "# extract_text_features(test_en_input, tokenizer, model, test_en_output)\n",
    "# extract_text_features(test_md_input, tokenizer, model, test_md_output)\n",
    "# extract_text_features(test_ar_input, tokenizer, model, test_ar_output)\n",
    "# extract_text_features(test_bg_input, tokenizer, model, test_bg_output)\n",
    "\n",
    "extract_text_features(dev_en_input, tokenizer, model, dev_en_output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e48fc0f7a09d7e3",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask2a/multilingual-ner/en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\", \n",
    "                                                        output_hidden_states=True)\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "\n",
    "dir = './TextFeatures/subtask2a/multilingual-ner/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask2a/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask2a/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask2a_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask2a_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask2a_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask2a_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "dev_en_input = './semeval2024_dev_release/subtask1/dev_unlabeled.json'\n",
    "dev_en_output = dir + 'en_dev_text_features.pkl'\n",
    "\n",
    "# extract_text_features(train_input, tokenizer, model, train_output)\n",
    "# extract_text_features(val_input, tokenizer, model, val_output)\n",
    "\n",
    "# extract_text_features(test_en_input, tokenizer, model, test_en_output)\n",
    "# extract_text_features(test_md_input, tokenizer, model, test_md_output)\n",
    "# extract_text_features(test_ar_input, tokenizer, model, test_ar_output)\n",
    "# extract_text_features(test_bg_input, tokenizer, model, test_bg_output)\n",
    "\n",
    "extract_text_features(dev_en_input, tokenizer, model, dev_en_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T00:12:24.580541Z",
     "start_time": "2024-04-29T00:12:13.042753Z"
    }
   },
   "id": "91f0b2a2cafb8d6e",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "70c2072510594301"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
