{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting POS tags (Experimenting)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7a153b5145d5d81"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. Italian                    2. French                     3. Spanish; Castilian       \n",
      "  4. Bulgarian                  5. Slovene                    6. Irish                    \n",
      "  7. Finnish                    8. Dutch                      9. Swedish                  \n",
      " 10. Danish                    11. Portuguese                12. English                  \n",
      " 13. German                    14. Indonesian                15. Czech                    \n",
      " 16. Hungarian                \n",
      "[polyglot_data] Error loading pos2.mk: Package 'pos2.mk' not found in\n",
      "[polyglot_data]     index\n"
     ]
    }
   ],
   "source": [
    "import polyglot\n",
    "from polyglot.downloader import downloader\n",
    "\n",
    "print(downloader.supported_languages_table(\"pos2\", 3))\n",
    "downloader.download(\"pos2.mk\")\n",
    "\n",
    "from polyglot.text import Text\n",
    "\n",
    "def get_pos_tags_polyglot(text, lang='mk'):\n",
    "    poly_text = Text(text, hint_language_code=lang)\n",
    "    return ' '.join([f'{word}_{tag}' for word, tag in zip(poly_text.words, poly_text.pos_tags)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T05:28:51.329516Z",
     "start_time": "2024-05-07T05:28:50.046624Z"
    }
   },
   "id": "93470849cdc96b99",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[1;32m     24\u001B[0m english_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis is a sample sentence.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnglish:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mget_pos_tags_stanza\u001B[49m\u001B[43m(\u001B[49m\u001B[43menglish_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     27\u001B[0m arabic_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mهذا نص عربي للتحليل.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArabic:\u001B[39m\u001B[38;5;124m\"\u001B[39m, get_pos_tags_stanza(arabic_text, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mar\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "Cell \u001B[0;32mIn[2], line 16\u001B[0m, in \u001B[0;36mget_pos_tags_stanza\u001B[0;34m(text, lang)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_pos_tags_stanza\u001B[39m(text, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# Retrieve the appropriate NLP pipeline based on language\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m     nlp \u001B[38;5;241m=\u001B[39m \u001B[43mget_stanza_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     doc \u001B[38;5;241m=\u001B[39m nlp(text)\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# Generate POS tags for each token in the text\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[2], line 9\u001B[0m, in \u001B[0;36mget_stanza_pipeline\u001B[0;34m(lang)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_stanza_pipeline\u001B[39m(lang):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m lang \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m nlp_pipelines:\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;66;03m# Download the model if not already downloaded\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m         \u001B[43mstanza\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;66;03m# Create a Stanza pipeline for the specified language\u001B[39;00m\n\u001B[1;32m     11\u001B[0m         nlp_pipelines[lang] \u001B[38;5;241m=\u001B[39m stanza\u001B[38;5;241m.\u001B[39mPipeline(lang, processors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenize,pos\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/resources/common.py:574\u001B[0m, in \u001B[0;36mdownload\u001B[0;34m(lang, model_dir, package, processors, logging_level, verbose, resources_url, resources_branch, resources_version, model_url, proxies, download_json)\u001B[0m\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m download_json:\n\u001B[1;32m    573\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to skip downloading resources.json, but the file does not exist.  Downloading anyway\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 574\u001B[0m     \u001B[43mdownload_resources_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresources_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresources_branch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresources_version\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresources_filepath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    576\u001B[0m resources \u001B[38;5;241m=\u001B[39m load_resources_json(model_dir)\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lang \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m resources:\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/resources/common.py:454\u001B[0m, in \u001B[0;36mdownload_resources_json\u001B[0;34m(model_dir, resources_url, resources_branch, resources_version, resources_filepath, proxies)\u001B[0m\n\u001B[1;32m    452\u001B[0m     resources_filepath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresources.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    453\u001B[0m \u001B[38;5;66;03m# make request\u001B[39;00m\n\u001B[0;32m--> 454\u001B[0m \u001B[43mrequest_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresources_url\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresources_filepath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m    459\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/resources/common.py:152\u001B[0m, in \u001B[0;36mrequest_file\u001B[0;34m(url, path, proxies, md5, raise_for_status, log_info, alternate_md5)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tempfile\u001B[38;5;241m.\u001B[39mTemporaryDirectory(\u001B[38;5;28mdir\u001B[39m\u001B[38;5;241m=\u001B[39mbasedir) \u001B[38;5;28;01mas\u001B[39;00m temp:\n\u001B[1;32m    151\u001B[0m     temppath \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(temp, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(path)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m--> 152\u001B[0m     \u001B[43mdownload_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemppath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    153\u001B[0m     os\u001B[38;5;241m.\u001B[39mreplace(temppath, path)\n\u001B[1;32m    154\u001B[0m assert_file_exists(path, md5, alternate_md5)\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/stanza/resources/common.py:114\u001B[0m, in \u001B[0;36mdownload_file\u001B[0;34m(url, path, proxies, raise_for_status)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03mDownload a URL into a file as specified by `path`.\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    113\u001B[0m verbose \u001B[38;5;241m=\u001B[39m logger\u001B[38;5;241m.\u001B[39mlevel \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m20\u001B[39m]\n\u001B[0;32m--> 114\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    116\u001B[0m     file_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(r\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent-length\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/requests/adapters.py:486\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    483\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    501\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    787\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 790\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    803\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    806\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;66;03m# Trigger any extra validation we need to do.\u001B[39;00m\n\u001B[1;32m    466\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 467\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    469\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mconn\u001B[38;5;241m.\u001B[39mtimeout)\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:1092\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1090\u001B[0m \u001B[38;5;66;03m# Force connect early to allow us to validate the connection.\u001B[39;00m\n\u001B[1;32m   1091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[0;32m-> 1092\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_verified:\n\u001B[1;32m   1095\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1096\u001B[0m         (\n\u001B[1;32m   1097\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnverified HTTPS request is being made to host \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconn\u001B[38;5;241m.\u001B[39mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1102\u001B[0m         InsecureRequestWarning,\n\u001B[1;32m   1103\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/urllib3/connection.py:611\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    610\u001B[0m     sock: socket\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m|\u001B[39m ssl\u001B[38;5;241m.\u001B[39mSSLSocket\n\u001B[0;32m--> 611\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    612\u001B[0m     server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[1;32m    613\u001B[0m     tls_in_tls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/urllib3/connection.py:203\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \n\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03m:return: New socket connection.\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    210\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NameResolutionError(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m, e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Python Assignments/venv/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m source_address:\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n\u001B[1;32m     75\u001B[0m err \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Dictionary to hold the Stanza pipeline objects for each language\n",
    "nlp_pipelines = {}\n",
    "\n",
    "def get_stanza_pipeline(lang):\n",
    "    if lang not in nlp_pipelines:\n",
    "        # Download the model if not already downloaded\n",
    "        stanza.download(lang)\n",
    "        # Create a Stanza pipeline for the specified language\n",
    "        nlp_pipelines[lang] = stanza.Pipeline(lang, processors='tokenize,pos')\n",
    "    return nlp_pipelines[lang]\n",
    "\n",
    "def get_pos_tags_stanza(text, lang='en'):\n",
    "    # Retrieve the appropriate NLP pipeline based on language\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    # Generate POS tags for each token in the text\n",
    "    \n",
    "    pos_tags = ' '.join([f'{word.text}_{word.upos}' for sent in doc.sentences for word in sent.words])\n",
    "    return pos_tags\n",
    "\n",
    "# Example usage\n",
    "english_text = \"This is a sample sentence.\"\n",
    "print(\"English:\", get_pos_tags_stanza(english_text, 'en'))\n",
    "\n",
    "arabic_text = \"هذا نص عربي للتحليل.\"\n",
    "print(\"Arabic:\", get_pos_tags_stanza(arabic_text, 'ar'))\n",
    "\n",
    "bulgarian_text = \"Това е примерен текст за анализ.\"\n",
    "print(\"Bulgarian:\", get_pos_tags_stanza(bulgarian_text, 'bg'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T05:29:15.805836Z",
     "start_time": "2024-05-07T05:28:51.331477Z"
    }
   },
   "id": "5653ed7ab21c4616",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load JSON data from a file\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Save updated JSON data to a file\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Decode HTML entities\n",
    "    # text = html.unescape(text)\n",
    "    \n",
    "    # Replace \\\\n and \\n (new lines) with a space\n",
    "    # print(text)\n",
    "    # print()\n",
    "    text = re.sub(r'(\\\\n)+', ' ', text)\n",
    "    \n",
    "    # print(text)\n",
    "    # print()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove any remaining backslashes\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    \n",
    "    # print(text)\n",
    "    # print()\n",
    "    \n",
    "    # Remove digits - You can comment this line if you want to keep numbers\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Optionally, remove punctuation\n",
    "    # text = text.translate(str.maketrans('', '', punctuation))\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    # text = text.lower()\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # print(text)\n",
    "    # print()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Get POS tags using spaCy\n",
    "def get_pos_tags(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([f'{token}_{token.pos_}'  for token in doc])\n",
    "    # return [token.pos_ for token in doc]\n",
    "\n",
    "def get_pos_tags(text, lang='en'):\n",
    "    if lang in ['en', 'ar', 'bg']:  # supported by Stanza\n",
    "        return get_pos_tags_stanza(text, lang)\n",
    "    elif lang == 'mk':  # use Polyglot for Macedonian\n",
    "        return get_pos_tags_polyglot(text, lang)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"POS tagging not implemented for language: {lang}\")\n",
    "\n",
    "\n",
    "# Main function to process the data\n",
    "def process_data(filename, output_filename, lang='en'):\n",
    "    data = load_data(filename)\n",
    "    for item in data:\n",
    "        # Preprocess the text\n",
    "        preprocessed_text = preprocess_text(item['text'])\n",
    "        # Get POS tags\n",
    "        pos_tags = get_pos_tags(preprocessed_text, lang)\n",
    "        # Update the item with POS tags\n",
    "        item['text'] = pos_tags\n",
    "        # item['processed_text'] = preprocessed_text\n",
    "        \n",
    "    # Save updated data\n",
    "    save_data(data, output_filename)\n",
    "    return data\n",
    "\n",
    "# Example of usage\n",
    "if __name__ == \"__main__\":\n",
    "    filename = 'semeval2024_dev_release/subtask1/validation.json'\n",
    "    output_file_path = 'semeval2024_dev_release/subtask1/pos_validation.json'\n",
    "    \n",
    "    train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "    train_output = './semeval2024_dev_release/subtask1/POS/train.json'\n",
    "    \n",
    "    val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "    val_output = './semeval2024_dev_release/subtask1/POS/validation.json'\n",
    "    \n",
    "    test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "    test_en_output = './test_data/english/POS/en_subtask1_test_unlabeled.json'\n",
    "    \n",
    "    test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "    test_md_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_md.json'\n",
    "    \n",
    "    test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "    test_ar_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_ar.json'\n",
    "    \n",
    "    test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "    test_bg_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_bg.json'\n",
    "    \n",
    "    # print(json.dumps(updated_data, indent=4))\n",
    "    \n",
    "    updated_data = process_data(train_input, train_output, lang='en')\n",
    "    updated_data = process_data(val_input, val_output, lang='en')\n",
    "    updated_data = process_data(test_en_input, test_en_output, lang='en')\n",
    "    updated_data = process_data(test_bg_input, test_bg_output, lang='bg')\n",
    "    updated_data = process_data(test_ar_input, test_ar_output, lang='ar')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dd872adbb4f2c76",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "class NLPModelManager:\n",
    "    def __init__(self):\n",
    "        self.nlp_pipelines = {}\n",
    "\n",
    "    def get_pipeline(self, lang='en'):\n",
    "        if lang not in self.nlp_pipelines:\n",
    "            # Download the model if not already downloaded\n",
    "            stanza.download(lang)\n",
    "            # Create a Stanza pipeline for the specified language\n",
    "            self.nlp_pipelines[lang] = stanza.Pipeline(lang, processors='tokenize,pos', use_gpu=True)\n",
    "        return self.nlp_pipelines[lang]\n",
    "\n",
    "# Create a global instance of the NLPModelManager\n",
    "nlp_manager = NLPModelManager()\n",
    "\n",
    "def get_pos_tags_stanza(text, lang='en'):\n",
    "    # Retrieve the appropriate NLP pipeline based on language from the manager\n",
    "    nlp = nlp_manager.get_pipeline(lang)\n",
    "    doc = nlp(text)\n",
    "    # Generate POS tags for each token in the text\n",
    "    pos_tags = ' '.join([f'{word.text}_{word.upos}' for sent in doc.sentences for word in sent.words])\n",
    "    return pos_tags\n",
    "\n",
    "# Example usage\n",
    "english_text = \"This is a sample sentence.\"\n",
    "print(\"English:\", get_pos_tags_stanza(english_text, 'en'))\n",
    "\n",
    "arabic_text = \"هذا نص عربي للتحليل.\"\n",
    "print(\"Arabic:\", get_pos_tags_stanza(arabic_text, 'ar'))\n",
    "\n",
    "bulgarian_text = \"Това е примерен текст за анализ.\"\n",
    "print(\"Bulgarian:\", get_pos_tags_stanza(bulgarian_text, 'bg'))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "246f7be14882b67b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_text_item(item, lang='en'):\n",
    "    preprocessed_text = preprocess_text(item['text'])\n",
    "    pos_tags = get_pos_tags(preprocessed_text, lang)\n",
    "    item['text'] = pos_tags\n",
    "    return item\n",
    "\n",
    "def process_data_concurrently(data, lang='en'):\n",
    "    processed_items = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_item = {executor.submit(process_text_item, item, lang): item for item in data}\n",
    "        for future in as_completed(future_to_item):\n",
    "            processed_items.append(future.result())\n",
    "    return processed_items\n",
    "\n",
    "def batch_process(data, batch_size=10, lang='en'):\n",
    "    nlp = get_stanza_pipeline(lang)\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        texts = [preprocess_text(item['text']) for item in batch]\n",
    "        docs = nlp(texts)\n",
    "        for doc, item in zip(docs, batch):\n",
    "            pos_tags = ' '.join([f'{word.text}_{word.upos}' for sent in doc.sentences for word in sent.words])\n",
    "            item['text'] = pos_tags\n",
    "    return data\n",
    "\n",
    "# import cProfile\n",
    "# import pstats\n",
    "# \n",
    "# with cProfile.Profile() as pr:\n",
    "#     process_data(test_ar_input, test_ar_output, lang='ar')\n",
    "# \n",
    "# stats = pstats.Stats(pr)\n",
    "# stats.sort_stats(pstats.SortKey.TIME)\n",
    "# stats.print_stats()\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    filename = 'semeval2024_dev_release/subtask1/validation.json'\n",
    "    output_file_path = 'semeval2024_dev_release/subtask1/pos_validation.json'\n",
    "\n",
    "    train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "    train_output = './semeval2024_dev_release/subtask1/POS/train.json'\n",
    "\n",
    "    val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "    val_output = './semeval2024_dev_release/subtask1/POS/validation.json'\n",
    "\n",
    "    test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "    test_en_output = './test_data/english/POS/en_subtask1_test_unlabeled.json'\n",
    "\n",
    "    test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "    test_md_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_md.json'\n",
    "\n",
    "    test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "    test_ar_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_ar.json'\n",
    "\n",
    "    test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "    test_bg_output = './test_labels_ar_bg_md_version2/POS/test_subtask1_bg.json'\n",
    "\n",
    "    # print(json.dumps(updated_data, indent=4))\n",
    "\n",
    "    # updated_data = process_data(train_input, train_output, lang='en')\n",
    "    # updated_data = process_data(val_input, val_output, lang='en')\n",
    "    updated_data = process_data(test_en_input, test_en_output, lang='en')\n",
    "    updated_data = process_data(test_bg_input, test_bg_output, lang='bg')\n",
    "    updated_data = process_data(test_ar_input, test_ar_output, lang='ar')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a041f91fd35ede7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "da4281cfe2f6d34c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting NER embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ff47d38903a0ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import torchvision.models as models\n",
    "from utils.utils import *\n",
    "from utils.label_decoding import *\n",
    "from utils.HierarchicalLoss import *"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "661f00377194cb64",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\", output_hidden_states=True)\n",
    "\n",
    "# Encode the input text and obtain the model outputs\n",
    "text = \"Insert your text here.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "outputs = model(**inputs)\n",
    "hidden_states = outputs.hidden_states\n",
    "token_embeddings = hidden_states[-1][0] \n",
    "\n",
    "\n",
    "# Strategy 2: Use the [CLS] token embedding\n",
    "cls_embedding = token_embeddings[0]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e216a55eab5b3b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cls_embedding.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32cd4b3aa566dc25",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = get_device()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4db099ecce6e8648",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def process_json(file_path, techniques_to_level, hierarchy):\n",
    "    data_df = pd.read_json(file_path)\n",
    "    data_df['cleaned_text'] = data_df['text'].apply(replace_newlines_with_fullstop)\n",
    "    if 'link' in data_df.columns:\n",
    "        data_df.drop(columns=['link'], inplace=True)\n",
    "\n",
    "    # for level in ['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5']:\n",
    "    #     data_df[level] = pd.Series([[] for _ in range(len(data_df))], index=data_df.index)\n",
    "    \n",
    "    return data_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f86283fdbca15d28",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_text_features(file_path, tokenizer, text_model, output_file_path, subtask=1, device=torch.device(\"mps\")):\n",
    "    features_dict = {}\n",
    "    \n",
    "    if subtask == 1:\n",
    "        data = process_json(file_path, techniques_to_level_1, hierarchy_1)\n",
    "    else:\n",
    "        data = process_json(file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for id, text in zip(data['id'], data['cleaned_text']):\n",
    "        # print(data['text'], data['cleaned_text'])\n",
    "        # break\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True, \n",
    "                                  max_length=128, truncation=True, padding='max_length').to(device)\n",
    "        \n",
    "        # input_ids = encoded_input['input_ids'].to('cpu')\n",
    "        # decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "        # print(decoded_text)\n",
    "        # return\n",
    "        with torch.no_grad():\n",
    "            embeddings = text_model(**encoded_input)\n",
    "        features_dict[id] = embeddings.hidden_states[-1][:, 0, :].detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        # print(features_dict[id].shape, features_dict[id].dtype)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f'completed {step} steps')\n",
    "            \n",
    "    with open(f'{output_file_path}', 'wb') as f:\n",
    "        pickle.dump(features_dict, f)\n",
    "    \n",
    "    print(f\"Features extracted and stored in {output_file_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ed8634e790038b1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\", output_hidden_states=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dir = './TextFeatures/subtask1a/BERT-NER/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "dev_en_input = './semeval2024_dev_release/subtask1/dev_unlabeled.json'\n",
    "dev_en_output = dir + 'en_dev_text_features.pkl'\n",
    "\n",
    "# extract_text_features(train_input, tokenizer, model, train_output)\n",
    "# extract_text_features(val_input, tokenizer, model, val_output)\n",
    "# \n",
    "# extract_text_features(test_en_input, tokenizer, model, test_en_output)\n",
    "# extract_text_features(test_md_input, tokenizer, model, test_md_output)\n",
    "# extract_text_features(test_ar_input, tokenizer, model, test_ar_output)\n",
    "# extract_text_features(test_bg_input, tokenizer, model, test_bg_output)\n",
    "\n",
    "extract_text_features(dev_en_input, tokenizer, model, dev_en_output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92ffaf21e67d8adc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\",\n",
    "                                                        output_hidden_states=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "dir = './TextFeatures/subtask1a/multilingual-ner/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "dev_en_input = './semeval2024_dev_release/subtask1/dev_unlabeled.json'\n",
    "dev_en_output = dir + 'en_dev_text_features.pkl'\n",
    "\n",
    "# extract_text_features(train_input, tokenizer, model, train_output)\n",
    "# extract_text_features(val_input, tokenizer, model, val_output)\n",
    "# \n",
    "# extract_text_features(test_en_input, tokenizer, model, test_en_output)\n",
    "# extract_text_features(test_md_input, tokenizer, model, test_md_output)\n",
    "# extract_text_features(test_ar_input, tokenizer, model, test_ar_output)\n",
    "# extract_text_features(test_bg_input, tokenizer, model, test_bg_output)\n",
    "\n",
    "extract_text_features(dev_en_input, tokenizer, model, dev_en_output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e48fc0f7a09d7e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/wikineural-multilingual-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Babelscape/wikineural-multilingual-ner\", \n",
    "                                                        output_hidden_states=True)\n",
    "device = get_device()\n",
    "model.to(device)\n",
    "\n",
    "dir = './TextFeatures/subtask2a/multilingual-ner/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask2a/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask2a/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask2a_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask2a_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask2a_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask2a_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "dev_en_input = './semeval2024_dev_release/subtask1/dev_unlabeled.json'\n",
    "dev_en_output = dir + 'en_dev_text_features.pkl'\n",
    "\n",
    "# extract_text_features(train_input, tokenizer, model, train_output)\n",
    "# extract_text_features(val_input, tokenizer, model, val_output)\n",
    "\n",
    "# extract_text_features(test_en_input, tokenizer, model, test_en_output)\n",
    "# extract_text_features(test_md_input, tokenizer, model, test_md_output)\n",
    "# extract_text_features(test_ar_input, tokenizer, model, test_ar_output)\n",
    "# extract_text_features(test_bg_input, tokenizer, model, test_bg_output)\n",
    "\n",
    "extract_text_features(dev_en_input, tokenizer, model, dev_en_output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f0b2a2cafb8d6e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9c483a4a74477e6",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
