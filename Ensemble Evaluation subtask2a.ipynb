{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-27T11:31:03.137263Z",
     "start_time": "2024-04-27T11:31:01.411937Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import nltk\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.label_decoding import *\n",
    "from utils.HierarchicalLoss import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    def __init__(self, df, image_features_files, text_features_files):\n",
    "        super(TestDataSet, self).__init__()\n",
    "        self.data_df = df\n",
    "        \n",
    "        self.image_features_dicts = []\n",
    "        \n",
    "        for idx in range(len(image_features_files)):\n",
    "            with open(image_features_files[idx], 'rb') as f:\n",
    "                self.image_features_dicts.append(pickle.load(f))\n",
    "        \n",
    "        \n",
    "        self.text_features_dicts = []\n",
    "        \n",
    "        for idx in range(len(text_features_files)):\n",
    "            with open(text_features_files[idx], 'rb') as f:\n",
    "                self.text_features_dicts.append(pickle.load(f))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id = self.data_df.iloc[idx]['id']\n",
    "        text = self.data_df.iloc[idx]['cleaned_text']\n",
    "        image_name = self.data_df.iloc[idx]['image']\n",
    "        \n",
    "        features = {f'image_features_{idx}': self.image_features_dicts[idx][image_name] for idx in range(len(self.image_features_dicts))}\n",
    "        \n",
    "        for idx in range(len(self.image_features_dicts)):\n",
    "            features[f'text_features_{idx}'] = self.text_features_dicts[idx][id]\n",
    "        \n",
    "        \n",
    "        features['id'] = id\n",
    "        features['text'] = text\n",
    "        \n",
    "        return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T11:31:03.142504Z",
     "start_time": "2024-04-27T11:31:03.138424Z"
    }
   },
   "id": "d51fa33cffdf6033",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ViT + OpenAI Small"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e9771e984ab7fd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules.nn.MultiModal import MultiModalBaseline\n",
    "\n",
    "vit_openai_small = MultiModalBaseline(img_feature_size=512, text_feature_size=1536)\n",
    "# vit_openai_small.load_state_dict(torch.load(\"models/subtask2a/ViT-OpenAI-Small/vit_openai_small.pt\"))\n",
    "vit_openai_small.load_state_dict(torch.load(\"models/subtask2a/MultiModal-OpenAI-Small/splendid-sweep-4.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:38:32.040100Z",
     "start_time": "2024-04-28T08:38:31.744039Z"
    }
   },
   "id": "c0c35582a4299cbe",
   "execution_count": 78
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ViT + OpenAI Large"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c359aaa7e5359b2c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules.nn.MultiModal import MultiModalBaseline\n",
    "\n",
    "vit_openai_large = MultiModalBaseline(img_feature_size=512, text_feature_size=3072)\n",
    "# vit_openai_large.load_state_dict(torch.load(\"models/subtask2a/ViT-OpenAI-Large/vit_openai_large.pt\"))\n",
    "vit_openai_large.load_state_dict(torch.load(\"models/subtask2a/MultiModal-OpenAI-Large/fresh-sweep-3.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:38:53.590808Z",
     "start_time": "2024-04-28T08:38:53.164872Z"
    }
   },
   "id": "e5c31742b6f8fc9b",
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ViT + OpenAI Large + ner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4003f50b800ba7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules.nn.MultiModal import MultiModalNER\n",
    "openai_large_ner = MultiModalNER(512, 3072, 768)\n",
    "\n",
    "openai_large_ner.load_state_dict(torch.load(\"models/subtask2a/MultiModal-OpenAI-Large-NER/polar-sweep-1.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:36:32.884203Z",
     "start_time": "2024-04-28T08:36:32.381924Z"
    }
   },
   "id": "caafe6a4b8e59912",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T11:31:18.042541Z",
     "start_time": "2024-04-27T11:31:18.040364Z"
    }
   },
   "id": "bab05b01756ba1db",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "def evaluate_models(models, dataloader, pred_file_path, gold_file_path, \n",
    "                    evaluator_script_path, id2leaf_label, device, validation=False, format=None,\n",
    "                    threshold=0.3):\n",
    "    # Ensure all models are in evaluation mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    total_loss = 0 if validation else None\n",
    "    HL = HierarchicalLoss(id2label=id2label_subtask_2a, hierarchical_labels=hierarchy_subtask_2a, persuasion_techniques=persuasion_techniques_2a, device=device)\n",
    "    \n",
    "    # model1, model2 = models\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            ids = batch['id'].detach().numpy().tolist() if not isinstance(batch['id'], list) else batch['id']\n",
    "            \n",
    "            # img_features1 = batch['img_features1'].to(device)\n",
    "            # text_features1 = batch['text_features1'].to(device)\n",
    "            # \n",
    "            # img_features2 = batch['img_features2'].to(device)\n",
    "            # text_features2 = batch['text_features2'].to(device)\n",
    "            \n",
    "            # Collect predictions from all models\n",
    "            batch_predictions = []\n",
    "            \n",
    "            for idx in range(len(models)):\n",
    "                model = models[idx]\n",
    "                if idx != 2:\n",
    "                    img_features = batch[f'image_features_{idx}'].to(device)\n",
    "                    text_features = batch[f'text_features_{idx}'].to(device)\n",
    "                    batch_predictions.append(model(text_features, img_features))\n",
    "                else:\n",
    "                    img_features = batch[f'image_features_{idx}'].to(device)\n",
    "                    text_features = batch[f'text_features_{idx-1}'].to(device)\n",
    "                    ner_features = batch[f'text_features_{idx}'].to(device)\n",
    "                    \n",
    "                    batch_predictions.append(model(text_features, img_features, ner_features))\n",
    "            \n",
    "            # Soft voting: average the predictions across models\n",
    "            # Assuming outputs are logits, use softmax to convert to probabilities\n",
    "            avg_preds = [torch.stack([model_preds[j] for model_preds in batch_predictions]).mean(0) for j in range(5)]\n",
    "            \n",
    "            pred_1, pred_2, pred_3, pred_4, pred_5 = avg_preds\n",
    "            \n",
    "            if validation:\n",
    "                y_1, y_2, y_3 = batch['level_1_target'], batch['level_2_target'], batch['level_3_target']\n",
    "                y_4, y_5 = batch['level_4_target'], batch['level_5_target']\n",
    "                y_1, y_2, y_3, y_4, y_5 = y_1.to(device), y_2.to(device), y_3.to(device), y_4.to(device), y_5.to(device)\n",
    "                \n",
    "                dloss = HL.calculate_dloss([pred_1, pred_2, pred_3, pred_4, pred_5], [y_1, y_2, y_3, y_4, y_5])\n",
    "                lloss = HL.calculate_lloss([pred_1, pred_2, pred_3, pred_4, pred_5], [y_1, y_2, y_3, y_4, y_5])\n",
    "                total_loss += (dloss + lloss).item()\n",
    "\n",
    "            # Threshold predictions for classification\n",
    "            pred_3 = (pred_3 > threshold).int().cpu().numpy()\n",
    "            pred_4 = (pred_4 > threshold).int().cpu().numpy()\n",
    "            pred_5 = (pred_5 > threshold).int().cpu().numpy()\n",
    "            # print(ids)\n",
    "            predictions += get_labels(id2leaf_label, ids, pred_3, pred_4, pred_5, format)\n",
    "\n",
    "    # Writing JSON data\n",
    "    with open(pred_file_path, 'w') as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "    \n",
    "    if gold_file_path is not None:\n",
    "        command = [\"python3\", evaluator_script_path, \"--gold_file_path\", gold_file_path, \"--pred_file_path\", pred_file_path]\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"Output:\\n\", result.stdout)\n",
    "        else:\n",
    "            print(\"Error:\\n\", result.stderr)\n",
    "    \n",
    "    if validation:\n",
    "        return total_loss / len(dataloader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T11:38:31.198845Z",
     "start_time": "2024-04-27T11:38:31.190643Z"
    }
   },
   "id": "ac9c1ba43f0f9b08",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MultiModal OpenAI Large and MultiModal OpenAI Small"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19a9dd66f1fbc28c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 23.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " f1_h=0.53272\tprec_h=0.49157\trec_h=0.58140\n"
     ]
    }
   ],
   "source": [
    "ar_pred_file_path = './Predictions/subtask2a/ar_predictions_subtask2a.txt'\n",
    "ar_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_ar.json'\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "image_features_files = ['ImageFeatures/CLIP-ViT/ar_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/ar_test_images_features.pkl']\n",
    "\n",
    "text_features_files = ['TextFeatures/subtask2a/text-embedding-3-small/ar_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/text-embedding-3-large/ar_test_text_features.pkl']\n",
    "\n",
    "ar_test_data = process_json(ar_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "ar_test_dataset = TestDataSet(ar_test_data, image_features_files, text_features_files)\n",
    "\n",
    "ar_test_dataloader =  DataLoader(ar_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_models(models=[vit_openai_small, vit_openai_large], \n",
    "                dataloader=ar_test_dataloader, pred_file_path=ar_pred_file_path, \n",
    "                gold_file_path=ar_gold_file_path, evaluator_script_path=evaluator_script, \n",
    "                device=torch.device('cpu'),\n",
    "               id2leaf_label=id2leaf_label_subtask_2a, format=5, validation=False, threshold=0.35)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:38:58.145834Z",
     "start_time": "2024-04-28T08:38:57.346585Z"
    }
   },
   "id": "5caebd09daeb7836",
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 26.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " f1_h=0.65156\tprec_h=0.65382\trec_h=0.64931\n"
     ]
    }
   ],
   "source": [
    "bg_pred_file_path = './Predictions/subtask2a/bg_predictions_subtask2a.txt'\n",
    "bg_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_bg.json'\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "bg_test_data = process_json(bg_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "\n",
    "image_features_files = ['ImageFeatures/CLIP-ViT/bulgarian_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/bulgarian_test_images_features.pkl']\n",
    "\n",
    "text_features_files = ['TextFeatures/subtask2a/text-embedding-3-small/bg_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/text-embedding-3-large/bg_test_text_features.pkl']\n",
    "\n",
    "\n",
    "bg_test_dataset = TestDataSet(df=bg_test_data, image_features_files=image_features_files,\n",
    "                              text_features_files=text_features_files)\n",
    "\n",
    "bg_test_dataloader =  DataLoader(bg_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_models(models=[vit_openai_small, vit_openai_large], \n",
    "                dataloader=bg_test_dataloader, pred_file_path=bg_pred_file_path, \n",
    "                gold_file_path=bg_gold_file_path, evaluator_script_path=evaluator_script, \n",
    "                device=torch.device('cpu'),\n",
    "               id2leaf_label=id2leaf_label_subtask_2a, format=None, validation=False, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:39:03.293476Z",
     "start_time": "2024-04-28T08:39:02.308836Z"
    }
   },
   "id": "daf626576b992e59",
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 26.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " f1_h=0.69425\tprec_h=0.74139\trec_h=0.65275\n"
     ]
    }
   ],
   "source": [
    "md_pred_file_path = './Predictions/subtask2a/md_predictions_subtask2a.txt'\n",
    "md_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_md.json'\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "md_test_data = process_json(md_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "\n",
    "image_features_files = ['ImageFeatures/CLIP-ViT/nm_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/nm_test_images_features.pkl',]\n",
    "text_features_files = ['TextFeatures/subtask2a/text-embedding-3-small/md_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/text-embedding-3-large/md_test_text_features.pkl']\n",
    "\n",
    "md_test_dataset = TestDataSet(df=md_test_data, image_features_files=image_features_files,\n",
    "                              text_features_files=text_features_files)\n",
    "\n",
    "md_test_dataloader =  DataLoader(md_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_models(models=[vit_openai_small, vit_openai_large], \n",
    "                dataloader=md_test_dataloader, pred_file_path=md_pred_file_path, \n",
    "                gold_file_path=md_gold_file_path, evaluator_script_path=evaluator_script, \n",
    "                device=torch.device('cpu'),\n",
    "               id2leaf_label=id2leaf_label_subtask_2a, format=None, validation=False, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:39:09.045647Z",
     "start_time": "2024-04-28T08:39:08.213549Z"
    }
   },
   "id": "a1417856b34aae4d",
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:02<00:00, 11.69it/s]\n"
     ]
    }
   ],
   "source": [
    "en_pred_file_path = './Predictions/subtask2a/en_predictions_subtask2a.txt'\n",
    "\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "en_test_data = process_test_json('test_data/english/en_subtask2a_test_unlabeled.json')\n",
    "\n",
    "image_features_files = ['ImageFeatures/CLIP-ViT/english_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/english_test_images_features.pkl']\n",
    "\n",
    "text_features_files = ['TextFeatures/subtask2a/text-embedding-3-small/en_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/text-embedding-3-large/en_test_text_features.pkl']\n",
    "\n",
    "\n",
    "en_test_dataset = TestDataSet(df=en_test_data, image_features_files=image_features_files, \n",
    "                              text_features_files=text_features_files)\n",
    "\n",
    "en_test_dataloader =  DataLoader(en_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_models(models=[vit_openai_small, vit_openai_large], \n",
    "                dataloader=en_test_dataloader, pred_file_path=en_pred_file_path, \n",
    "                evaluator_script_path=evaluator_script, gold_file_path=None,\n",
    "                device=torch.device('cpu'),\n",
    "               id2leaf_label=id2leaf_label_subtask_2a, format=None, validation=False, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T11:28:13.077183Z",
     "start_time": "2024-04-27T11:28:10.942694Z"
    }
   },
   "id": "a6c647b52ea2b6bb",
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "0.68318\t0.71913\t0.65065"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e5639272365e3a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MultiModal OpenAI Large + MultiModal OpenAI Small + OpenAI Large with NER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cffc0ad6f06d3dc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " f1_h=0.53378\tprec_h=0.53601\trec_h=0.53156\n"
     ]
    }
   ],
   "source": [
    "ar_pred_file_path = './Predictions/subtask2a/ar_predictions_subtask2a.txt'\n",
    "ar_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_ar.json'\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "image_features_files = ['ImageFeatures/CLIP-ViT/ar_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/ar_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/ar_test_images_features.pkl']\n",
    "\n",
    "text_features_files = ['TextFeatures/subtask2a/text-embedding-3-small/ar_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/text-embedding-3-large/ar_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/multilingual-ner/ar_test_text_features.pkl']\n",
    "\n",
    "ar_test_data = process_json(ar_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "ar_test_dataset = TestDataSet(ar_test_data, image_features_files, text_features_files)\n",
    "\n",
    "ar_test_dataloader =  DataLoader(ar_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_models(models=[vit_openai_small, vit_openai_large, openai_large_ner], \n",
    "                dataloader=ar_test_dataloader, pred_file_path=ar_pred_file_path, \n",
    "                gold_file_path=ar_gold_file_path, evaluator_script_path=evaluator_script, \n",
    "                device=torch.device('cpu'),\n",
    "               id2leaf_label=id2leaf_label_subtask_2a, format=5, validation=False, threshold=0.35)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T08:39:20.894641Z",
     "start_time": "2024-04-28T08:39:19.968864Z"
    }
   },
   "id": "7a13fd1f6ab01b6f",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " f1_h=0.65638\tprec_h=0.67555\trec_h=0.63828\n"
     ]
    }
   ],
   "source": [
    "bg_pred_file_path = './Predictions/subtask2a/bg_predictions_subtask2a.txt'\n",
    "bg_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_bg.json'\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "bg_test_data = process_json(bg_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "\n",
    "image_features_files = ['ImageFeatures/CLIP-ViT/bulgarian_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/bulgarian_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/bulgarian_test_images_features.pkl']\n",
    "\n",
    "text_features_files = ['TextFeatures/subtask2a/text-embedding-3-small/bg_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/text-embedding-3-large/bg_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/multilingual-ner/bg_test_text_features.pkl']\n",
    "\n",
    "\n",
    "bg_test_dataset = TestDataSet(df=bg_test_data, image_features_files=image_features_files,\n",
    "                              text_features_files=text_features_files)\n",
    "\n",
    "bg_test_dataloader =  DataLoader(bg_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_models(models=[vit_openai_small, vit_openai_large, openai_large_ner], \n",
    "                dataloader=bg_test_dataloader, pred_file_path=bg_pred_file_path, \n",
    "                gold_file_path=bg_gold_file_path, evaluator_script_path=evaluator_script, \n",
    "                device=torch.device('cpu'),\n",
    "               id2leaf_label=id2leaf_label_subtask_2a, format=None, validation=False, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T20:20:24.586368Z",
     "start_time": "2024-04-27T20:20:23.552360Z"
    }
   },
   "id": "3337a2b532d1561c",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " f1_h=0.69844\tprec_h=0.76676\trec_h=0.64130\n"
     ]
    }
   ],
   "source": [
    "md_pred_file_path = './Predictions/subtask2a/md_predictions_subtask2a.txt'\n",
    "md_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_md.json'\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "md_test_data = process_json(md_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "\n",
    "image_features_files = ['ImageFeatures/CLIP-ViT/nm_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/nm_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/nm_test_images_features.pkl']\n",
    "\n",
    "text_features_files = ['TextFeatures/subtask2a/text-embedding-3-small/md_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/text-embedding-3-large/md_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/multilingual-ner/md_test_text_features.pkl']\n",
    "\n",
    "md_test_dataset = TestDataSet(df=md_test_data, image_features_files=image_features_files,\n",
    "                              text_features_files=text_features_files)\n",
    "\n",
    "md_test_dataloader =  DataLoader(md_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_models(models=[vit_openai_small, vit_openai_large, openai_large_ner], \n",
    "                dataloader=md_test_dataloader, pred_file_path=md_pred_file_path, \n",
    "                gold_file_path=md_gold_file_path, evaluator_script_path=evaluator_script, \n",
    "                device=torch.device('cpu'),\n",
    "               id2leaf_label=id2leaf_label_subtask_2a, format=None, validation=False, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T20:20:29.556443Z",
     "start_time": "2024-04-27T20:20:28.582087Z"
    }
   },
   "id": "15ab9b0473fb343c",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:01<00:00, 13.97it/s]\n"
     ]
    }
   ],
   "source": [
    "en_pred_file_path = './Predictions/subtask2a/en_predictions_subtask2a.txt'\n",
    "\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "en_test_data = process_test_json('test_data/english/en_subtask2a_test_unlabeled.json')\n",
    "\n",
    "image_features_files = ['ImageFeatures/CLIP-ViT/english_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/english_test_images_features.pkl',\n",
    "                        'ImageFeatures/CLIP-ViT/english_test_images_features.pkl']\n",
    "\n",
    "text_features_files = ['TextFeatures/subtask2a/text-embedding-3-small/en_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/text-embedding-3-large/en_test_text_features.pkl',\n",
    "                       'TextFeatures/subtask2a/multilingual-ner/en_test_text_features.pkl']\n",
    "\n",
    "\n",
    "en_test_dataset = TestDataSet(df=en_test_data, image_features_files=image_features_files, \n",
    "                              text_features_files=text_features_files)\n",
    "\n",
    "en_test_dataloader =  DataLoader(en_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_models(models=[vit_openai_small, vit_openai_large, openai_large_ner], \n",
    "                dataloader=en_test_dataloader, pred_file_path=en_pred_file_path, \n",
    "                evaluator_script_path=evaluator_script, gold_file_path=None,\n",
    "                device=torch.device('cpu'),\n",
    "               id2leaf_label=id2leaf_label_subtask_2a, format=None, validation=False, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T00:03:55.981055Z",
     "start_time": "2024-04-28T00:03:54.087708Z"
    }
   },
   "id": "2a92090df361b96",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "The score for above submission is 0.69666\t0.73742\t0.66018"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3403b082d4e28f91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "49b1079c71e71df4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
