{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import torchvision.models as models\n",
    "from utils.utils import *\n",
    "from utils.label_decoding import *\n",
    "from utils.HierarchicalLoss import *"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T22:42:15.987777Z",
     "start_time": "2024-04-28T22:42:15.714852Z"
    }
   },
   "id": "81415241ae9d989",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = get_device()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b88f9f086c4298",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ResNet-50 Image Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a906110b93c8d73d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def process_folder(folder_path, model, transform):\n",
    "    \"\"\"Process all images in a folder and store their features in a dictionary\"\"\"\n",
    "    features_dict = {}\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        if os.path.isfile(image_path) and image_path.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif')):\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "            image = transform(image)\n",
    "            # Add a batch dimension\n",
    "            image = image.unsqueeze(0)\n",
    "            image = image.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                features = model(image)\n",
    "            features = features.cpu().squeeze().squeeze().numpy()\n",
    "            features_dict[image_name] = features\n",
    "    \n",
    "    return features_dict\n",
    "\n",
    "def extract_image_features(folder_path, modelname='resnet50', output_file_path='features.pkl'):\n",
    "  device = get_device()\n",
    "  model = None\n",
    "\n",
    "  if modelname == 'resnet50':\n",
    "\n",
    "    # Initialize the model\n",
    "    model = resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "  \n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "  model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  # Define a transform to preprocess the images\n",
    "  transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                  ])\n",
    "  \n",
    "\n",
    "  features_dict = process_folder(folder_path, model, transform)\n",
    "\n",
    "  with open(f'{output_file_path}', 'wb') as f:\n",
    "    pickle.dump(features_dict, f)\n",
    "\n",
    "  print(f\"Features extracted and stored in {output_file_path}\")\n"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_image_features('./test_images/subtask1_2a/english', \n",
    "                       output_file_path='./ImageFeatures/english_test_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c7fdee9bb2b937c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_image_features('./test_images/subtask1_2a/bulgarian', \n",
    "                       output_file_path='./ImageFeatures/bulgarian_test_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9969ee2bb68e802",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_image_features('./test_images/subtask1_2a/north_macedonian', \n",
    "                       output_file_path='./ImageFeatures/nm_test_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d81bedf64f776b53",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_image_features('./train_images', \n",
    "                       output_file_path='./ImageFeatures/train_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deef090e226f1443",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_image_features('./validation_images', \n",
    "                       output_file_path='./ImageFeatures/validation_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12f41471c7b76e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_image_features('./dev_images', \n",
    "                       output_file_path='./ImageFeatures/dev_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "321d1e384cbab529",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_image_features('./test_images_arabic/subtask2a', \n",
    "                       output_file_path='./ImageFeatures/ar_test_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83ba7db14beda67a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting Textual Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a887bef78aeb2509"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_text_features(file_path, tokenizer, text_model, output_file_path, subtask=1):\n",
    "    features_dict = {}\n",
    "    \n",
    "    if subtask == 1:\n",
    "        data = process_json(file_path, techniques_to_level_1, hierarchy_1)\n",
    "    else:\n",
    "        data = process_json(file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for id, text in zip(data['id'], data['cleaned_text']):\n",
    "        # print(data['text'], data['cleaned_text'])\n",
    "        # break\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True, \n",
    "                                  max_length=128, truncation=True, padding='max_length').to(device)\n",
    "        \n",
    "        # input_ids = encoded_input['input_ids'].to('cpu')\n",
    "        # decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "        # print(decoded_text)\n",
    "        # return\n",
    "        with torch.no_grad():\n",
    "            embeddings = text_model(**encoded_input)\n",
    "        features_dict[id] = embeddings.last_hidden_state[:, 0, :].detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f'completed {step} steps')\n",
    "            \n",
    "    with open(f'{output_file_path}', 'wb') as f:\n",
    "        pickle.dump(features_dict, f)\n",
    "    \n",
    "    print(f\"Features extracted and stored in {output_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T22:42:27.050040Z",
     "start_time": "2024-04-28T22:42:27.046807Z"
    }
   },
   "id": "15ca8b699ec5f9b6",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mBERT for subtask-1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1106ffb8f2320545"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/mBERT/en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "device = get_device()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "text_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\").to(device)\n",
    "\n",
    "\n",
    "dir = './TextFeatures/subtask1a/mBERT/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "dev_en_input = './semeval2024_dev_release/subtask1/dev_unlabeled.json'\n",
    "dev_en_output = dir + 'en_dev_text_features.pkl'\n",
    "\n",
    "\n",
    "extract_text_features(train_input, tokenizer, text_model, train_output)\n",
    "extract_text_features(val_input, tokenizer, text_model, val_output)\n",
    "\n",
    "def process_json(file_path, techniques_to_level, hierarchy):\n",
    "    data_df = pd.read_json(file_path)\n",
    "    data_df['cleaned_text'] = data_df['text'].apply(replace_newlines_with_fullstop)\n",
    "    if 'link' in data_df.columns:\n",
    "        data_df.drop(columns=['link'], inplace=True)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "extract_text_features(test_en_input, tokenizer, text_model, test_en_output)\n",
    "extract_text_features(test_md_input, tokenizer, text_model, test_md_output)\n",
    "extract_text_features(test_ar_input, tokenizer, text_model, test_ar_output)\n",
    "extract_text_features(test_bg_input, tokenizer, text_model, test_bg_output)\n",
    "extract_text_features(dev_en_input, tokenizer, text_model, dev_en_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T22:42:41.785425Z",
     "start_time": "2024-04-28T22:42:31.078834Z"
    }
   },
   "id": "77de3c7558c526cb",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### mBERT for subtask 2a"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e4983b6c8b0778f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "text_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
    "\n",
    "\n",
    "dir = './TextFeatures/subtask2a/mBERT/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask2a/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask2a/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask2a_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask2a_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask2a_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask2a_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "\n",
    "extract_text_features(train_input, tokenizer, text_model, train_output, subtask=2)\n",
    "extract_text_features(val_input, tokenizer, text_model, val_output, subtask=2)\n",
    "\n",
    "\n",
    "def process_json(file_path, techniques_to_level, hierarchy):\n",
    "    data_df = pd.read_json(file_path)\n",
    "    data_df['cleaned_text'] = data_df['text'].apply(replace_newlines_with_fullstop)\n",
    "    if 'link' in data_df.columns:\n",
    "        data_df.drop(columns=['link'], inplace=True)\n",
    "\n",
    "    for level in ['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5']:\n",
    "        data_df[level] = pd.Series([[] for _ in range(len(data_df))], index=data_df.index)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "extract_text_features(test_en_input, tokenizer, text_model, test_en_output, subtask=2)\n",
    "extract_text_features(test_md_input, tokenizer, text_model, test_md_output, subtask=2)\n",
    "extract_text_features(test_ar_input, tokenizer, text_model, test_ar_output, subtask=2)\n",
    "extract_text_features(test_bg_input, tokenizer, text_model, test_bg_output, subtask=2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b90e975a60439dc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### XLM BERT Features for subtask 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e89c3b0924c5ff5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/XLM-RoBERTa/en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "\n",
    "model_name = 'xlm-roberta-large'\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "model = XLMRobertaModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "dir = './TextFeatures/subtask1a/XLM-RoBERTa/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "dev_en_input = './semeval2024_dev_release/subtask1/dev_unlabeled.json'\n",
    "dev_en_output = dir + 'en_dev_text_features.pkl'\n",
    "\n",
    "extract_text_features(train_input, tokenizer, model, train_output)\n",
    "extract_text_features(val_input, tokenizer, model, val_output)\n",
    "\n",
    "def process_json(file_path, techniques_to_level, hierarchy):\n",
    "    data_df = pd.read_json(file_path)\n",
    "    data_df['cleaned_text'] = data_df['text'].apply(replace_newlines_with_fullstop)\n",
    "    if 'link' in data_df.columns:\n",
    "        data_df.drop(columns=['link'], inplace=True)\n",
    "\n",
    "    for level in ['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5']:\n",
    "        data_df[level] = pd.Series([[] for _ in range(len(data_df))], index=data_df.index)\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "extract_text_features(test_en_input, tokenizer, model, test_en_output)\n",
    "extract_text_features(test_md_input, tokenizer, model, test_md_output)\n",
    "extract_text_features(test_ar_input, tokenizer, model, test_ar_output)\n",
    "extract_text_features(test_bg_input, tokenizer, model, test_bg_output)\n",
    "extract_text_features(dev_en_input, tokenizer, model, dev_en_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T22:51:01.455993Z",
     "start_time": "2024-04-28T22:50:35.790296Z"
    }
   },
   "id": "479c45f8e7311c3f",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iqbal/Desktop/Python Assignments/venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLNetModel, XLNetTokenizer\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "model = XLNetModel.from_pretrained('xlnet-large-cased')\n",
    "\n",
    "# Encode some text\n",
    "text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass, get hidden states\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# The last hidden-state is the first element of the outputs tuple\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# You can now use last_hidden_states for various downstream tasks\n",
    "# It is a tensor of shape [batch_size, sequence_length, hidden_size]\n",
    "print(last_hidden_states.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:20:16.167833Z",
     "start_time": "2024-04-27T22:20:13.991450Z"
    }
   },
   "id": "7f286ba472565bd1",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_text_features(file_path, tokenizer, text_model, output_file_path, subtask=1):\n",
    "    features_dict = {}\n",
    "    \n",
    "    if subtask == 1:\n",
    "        data = process_json(file_path, techniques_to_level_1, hierarchy_1)\n",
    "    else:\n",
    "        data = process_json(file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for id, text in zip(data['id'], data['cleaned_text']):\n",
    "        # print(data['text'], data['cleaned_text'])\n",
    "        # break\n",
    "        inputs = tokenizer(text, return_tensors='pt', add_special_tokens=True, \n",
    "                                  max_length=128, truncation=True, padding='max_length').to(device)\n",
    "        \n",
    "        # input_ids = encoded_input['input_ids'].to('cpu')\n",
    "        # decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "        # print(decoded_text)\n",
    "        # return\n",
    "        with torch.no_grad():\n",
    "            embeddings = text_model(**inputs)\n",
    "            \n",
    "        last_hidden_states = embeddings.last_hidden_state\n",
    "            \n",
    "        attention_mask = inputs['attention_mask']\n",
    "        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_states * mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "        mean_pooled_embeddings = sum_embeddings / sum_mask\n",
    "        features_dict[id] = mean_pooled_embeddings.detach().cpu().squeeze().numpy()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f'completed {step} steps')\n",
    "            \n",
    "    with open(f'{output_file_path}', 'wb') as f:\n",
    "        pickle.dump(features_dict, f)\n",
    "    \n",
    "    print(f\"Features extracted and stored in {output_file_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f858ffb5b25327a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n",
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "completed 1100 steps\n",
      "completed 1200 steps\n",
      "completed 1300 steps\n",
      "completed 1400 steps\n",
      "completed 1500 steps\n",
      "completed 1600 steps\n",
      "completed 1700 steps\n",
      "completed 1800 steps\n",
      "completed 1900 steps\n",
      "completed 2000 steps\n",
      "completed 2100 steps\n",
      "completed 2200 steps\n",
      "completed 2300 steps\n",
      "completed 2400 steps\n",
      "completed 2500 steps\n",
      "completed 2600 steps\n",
      "completed 2700 steps\n",
      "completed 2800 steps\n",
      "completed 2900 steps\n",
      "completed 3000 steps\n",
      "completed 3100 steps\n",
      "completed 3200 steps\n",
      "completed 3300 steps\n",
      "completed 3400 steps\n",
      "completed 3500 steps\n",
      "completed 3600 steps\n",
      "completed 3700 steps\n",
      "completed 3800 steps\n",
      "completed 3900 steps\n",
      "completed 4000 steps\n",
      "completed 4100 steps\n",
      "completed 4200 steps\n",
      "completed 4300 steps\n",
      "completed 4400 steps\n",
      "completed 4500 steps\n",
      "completed 4600 steps\n",
      "completed 4700 steps\n",
      "completed 4800 steps\n",
      "completed 4900 steps\n",
      "completed 5000 steps\n",
      "completed 5100 steps\n",
      "completed 5200 steps\n",
      "completed 5300 steps\n",
      "completed 5400 steps\n",
      "completed 5500 steps\n",
      "completed 5600 steps\n",
      "completed 5700 steps\n",
      "completed 5800 steps\n",
      "completed 5900 steps\n",
      "completed 6000 steps\n",
      "completed 6100 steps\n",
      "completed 6200 steps\n",
      "completed 6300 steps\n",
      "completed 6400 steps\n",
      "completed 6500 steps\n",
      "completed 6600 steps\n",
      "completed 6700 steps\n",
      "completed 6800 steps\n",
      "completed 6900 steps\n",
      "completed 7000 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/XLNet/train_text_features.pkl\n",
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/XLNet/validation_text_features.pkl\n",
      "completed 100 steps\n",
      "completed 200 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/XLNet/md_test_text_features.pkl\n",
      "completed 100 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/XLNet/ar_test_text_features.pkl\n",
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "Features extracted and stored in ./TextFeatures/subtask1a/XLNet/bg_test_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import XLNetModel, XLNetTokenizer\n",
    "\n",
    "device = get_device()\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "text_model = XLNetModel.from_pretrained('xlnet-large-cased').to(device)\n",
    "\n",
    "\n",
    "dir = './TextFeatures/subtask1a/XLNet/'\n",
    "\n",
    "train_input = './semeval2024_dev_release/subtask1/train.json'\n",
    "train_output = dir + 'train_text_features.pkl'\n",
    "\n",
    "val_input = './semeval2024_dev_release/subtask1/validation.json'\n",
    "val_output = dir + 'validation_text_features.pkl'\n",
    "\n",
    "test_en_input = './test_data/english/en_subtask1_test_unlabeled.json'\n",
    "test_en_output = dir + 'en_test_text_features.pkl'\n",
    "\n",
    "test_md_input = './test_labels_ar_bg_md_version2/test_subtask1_md.json'\n",
    "test_md_output = dir + 'md_test_text_features.pkl'\n",
    "\n",
    "test_ar_input = './test_labels_ar_bg_md_version2/test_subtask1_ar.json'\n",
    "test_ar_output = dir + 'ar_test_text_features.pkl'\n",
    "\n",
    "test_bg_input = './test_labels_ar_bg_md_version2/test_subtask1_bg.json'\n",
    "test_bg_output = dir + 'bg_test_text_features.pkl'\n",
    "\n",
    "extract_text_features(train_input, tokenizer, text_model, train_output)\n",
    "extract_text_features(val_input, tokenizer, text_model, val_output)\n",
    "\n",
    "def process_json(file_path, techniques_to_level, hierarchy):\n",
    "    data_df = pd.read_json(file_path)\n",
    "    data_df['cleaned_text'] = data_df['text'].apply(replace_newlines_with_fullstop)\n",
    "    if 'link' in data_df.columns:\n",
    "        data_df.drop(columns=['link'], inplace=True)\n",
    "\n",
    "    for level in ['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5']:\n",
    "        data_df[level] = pd.Series([[] for _ in range(len(data_df))], index=data_df.index)\n",
    "    \n",
    "    return data_df\n",
    "# extract_text_features(test_en_input, tokenizer, text_model, test_en_output)\n",
    "extract_text_features(test_md_input, tokenizer, text_model, test_md_output)\n",
    "extract_text_features(test_ar_input, tokenizer, text_model, test_ar_output)\n",
    "extract_text_features(test_bg_input, tokenizer, text_model, test_bg_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:33:53.450443Z",
     "start_time": "2024-04-27T22:27:03.478762Z"
    }
   },
   "id": "46896118c0ce7081",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting CLIP+ViT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d6c8a96481d0bc6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model\n",
    "device = torch.device('mps')\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "836e66e6c7804f8d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load an image\n",
    "\n",
    "\n",
    "image_path = 'test_images/subtask1_2a/english/prop_meme_2.png'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "image = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Encode image using the CLIP model\n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "    # Optionally, you might want to normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "print(image_features.squeeze().shape)  \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42ce5a16a66a2061",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_clip_vit_features(folder_path, output_file_path):\n",
    "    device = torch.device('mps')\n",
    "    model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "    features_dict = {}\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        if os.path.isfile(image_path) and image_path.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif')):\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # Preprocess the image\n",
    "            image = preprocess(image).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Encode image using the CLIP model\n",
    "                image_features = model.encode_image(image)\n",
    "            \n",
    "                # Optionally, you might want to normalize the features\n",
    "                image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "                features_dict[image_name] = image_features.cpu().squeeze().numpy()\n",
    "    \n",
    "    with open(f'{output_file_path}', 'wb') as f:\n",
    "        pickle.dump(features_dict, f)\n",
    "\n",
    "    print(f\"Features extracted and stored in {output_file_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90e988dd4431d76b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_clip_vit_features('./test_images/subtask1_2a/english', './ImageFeatures/CLIP-ViT/english_test_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e1f9a3d089f9cd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_clip_vit_features('./test_images/subtask1_2a/bulgarian',\n",
    "                          './ImageFeatures/CLIP-ViT/bulgarian_test_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "279fbcc4eb6bc726",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_clip_vit_features('./test_images/subtask1_2a/north_macedonian',\n",
    "                          './ImageFeatures/CLIP-ViT/nm_test_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c8f2854de6ee5ce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_clip_vit_features('./test_images_arabic/subtask2a',\n",
    "                          './ImageFeatures/CLIP-ViT/ar_test_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0c757d48a35b532",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_clip_vit_features('./train_images', \n",
    "                          './ImageFeatures/CLIP-ViT/train_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77702fdb8e88e370",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "extract_clip_vit_features('./validation_images',\n",
    "                          './ImageFeatures/CLIP-ViT/validation_images_features.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed1b7921b71931ed",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting features from OpenAI's 3rd generation embedding models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "559a50999b208135"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='YourKEY')\n",
    "model=\"text-embedding-3-small\"\n",
    "emb = client.embeddings.create(input = ['Hello Every One'], model=model).data[0].embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T22:52:20.253688Z",
     "start_time": "2024-04-28T22:52:19.227385Z"
    }
   },
   "id": "ba30ecf2dbe0dd8",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def process_json(file_path):\n",
    "    data_df = pd.read_json(file_path)\n",
    "    data_df['cleaned_text'] = data_df['text'].apply(replace_newlines_with_fullstop)\n",
    "    if 'link' in data_df.columns:\n",
    "        data_df.drop(columns=['link'], inplace=True)\n",
    "\n",
    "    for level in ['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5']:\n",
    "        data_df[level] = pd.Series([[] for _ in range(len(data_df))], index=data_df.index)\n",
    "    \n",
    "    return data_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T22:52:22.355283Z",
     "start_time": "2024-04-28T22:52:22.352496Z"
    }
   },
   "id": "97c89f3c463edac2",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def extract_openai_features(file_path, model, output_file_path, task):\n",
    "    directory_path = f'TextFeatures/{task}/{model}' \n",
    "    \n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    client = OpenAI(api_key='YOURKEY')\n",
    "    features_dict = {}\n",
    "    \n",
    "    data = process_json(file_path)\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for id, text in zip(data['id'], data['cleaned_text']):\n",
    "        \n",
    "        try:\n",
    "            features_dict[id] = np.array(client.embeddings.create(input = [text], model=model).data[0].embedding, dtype=np.float32)\n",
    "        except:\n",
    "            print(f'exception for id: {id} and text : {text}')\n",
    "            features_dict[id] = np.zeros(1536, dtype=np.float32)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f'completed {step} steps')\n",
    "            \n",
    "    with open(f'{directory_path}/{output_file_path}', 'wb') as f:\n",
    "        pickle.dump(features_dict, f)\n",
    "    \n",
    "    print(f\"Features extracted and stored in {output_file_path}\")\n",
    "    \n",
    "    return features_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T22:52:28.449141Z",
     "start_time": "2024-04-28T22:52:28.445109Z"
    }
   },
   "id": "2107ae901dcb7887",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### text-embedding-3-large for subtask-1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71c2310f693a60c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask1/validation.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='validation_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a49499519db156c",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask1/train.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='train_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62aa01044d250b19",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_data/english/en_subtask1_test_unlabeled.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='en_test_text_features.pkl',\n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8601a6f7ac4ec410",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask1_bg.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='bg_test_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be201c561dab7fd3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask1_md.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='md_test_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e53c3674d37d9022",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask1_ar.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='ar_test_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c81ecb457eabd81a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask1/dev_unlabeled.json',\n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='en_dev_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T22:58:11.977366Z",
     "start_time": "2024-04-28T22:53:42.557265Z"
    }
   },
   "id": "c4ef24ced28ad2be",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask1/dev_unlabeled.json',\n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='en_dev_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-28T23:02:00.124184Z",
     "start_time": "2024-04-28T22:58:11.978767Z"
    }
   },
   "id": "50bb3b9c8d1df832",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### text-embedding-3-small for subtask-1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28cac6054a41d082"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask1/validation.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='validation_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2032560537e3008b",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask1/train.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='train_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f7b7f3fb980522",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_data/english/en_subtask1_test_unlabeled.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='en_test_text_features.pkl',\n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2291c44174a6d6c7",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask1_bg.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='bg_test_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b426f81f507dcd0",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask1_md.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='md_test_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f53c1e85a93d97a",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask1_ar.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='ar_test_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3c1dc4deceffc64",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask1/dev_unlabeled.json',\n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='en_dev_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cba2ed39bc607250",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 100 steps\n",
      "completed 200 steps\n",
      "completed 300 steps\n",
      "completed 400 steps\n",
      "completed 500 steps\n",
      "completed 600 steps\n",
      "completed 700 steps\n",
      "completed 800 steps\n",
      "completed 900 steps\n",
      "completed 1000 steps\n",
      "Features extracted and stored in en_dev_text_features.pkl\n"
     ]
    }
   ],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask1/dev_unlabeled.json',\n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='en_dev_text_features.pkl', \n",
    "                            task='subtask1a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7eca91aa5c485ab",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract text features using text-embeddings-3-small for subtask 2a"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5538754e58f393d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask2a/train.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='train_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32a4f27decb4dbb1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask2a/validation.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='validation_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b8f4fda42167f8d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_data/english/en_subtask2a_test_unlabeled.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='en_test_text_features.pkl',\n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8faea4ebad773b1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask2a_bg.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='bg_test_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "905c14fb36bd5e3e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask2a_md.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='md_test_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a08d9354301c9f4b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask2a_ar.json', \n",
    "                            model='text-embedding-3-small',\n",
    "                            output_file_path='ar_test_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f48215f209f0566",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract text features using text-embeddings-3-large for subtask 2a"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32df466417c6438f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask2a/train.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='train_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449710841463ee14",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./semeval2024_dev_release/subtask2a/validation.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='validation_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40f6173ec6bd0d6",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_data/english/en_subtask2a_test_unlabeled.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='en_test_text_features.pkl',\n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e91f6b215574e",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask2a_bg.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='bg_test_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3324c01a56c785dc",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask2a_md.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='md_test_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65db120ef215bbc7",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "d = extract_openai_features(file_path='./test_labels_ar_bg_md_version2/test_subtask2a_ar.json', \n",
    "                            model='text-embedding-3-large',\n",
    "                            output_file_path='ar_test_text_features.pkl', \n",
    "                            task='subtask2a')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf6c784f06f5fc7",
   "execution_count": 0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
