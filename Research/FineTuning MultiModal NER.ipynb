{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-27T10:31:57.066077Z",
     "start_time": "2024-04-27T10:31:54.972729Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import nltk\n",
    "from utils.utils import *\n",
    "from utils.label_decoding import *\n",
    "from utils.HierarchicalLoss import *\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import subprocess\n",
    "from subtask_1_2a import *"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "class DataSet(Dataset):\n",
    "    def __init__(self, df, labels_at_level, text_features_file, image_features_file,\n",
    "                 ner_features_file, max_len=128):\n",
    "        super(DataSet, self).__init__()\n",
    "        self.data_df = df\n",
    "        self.labels_at_level = labels_at_level\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        # self.image_folder = image_folder\n",
    "        self.image_features = None\n",
    "        self.text_features = None\n",
    "        \n",
    "        with open(image_features_file, 'rb') as f:\n",
    "          self.image_features = pickle.load(f)\n",
    "        \n",
    "        with open(text_features_file, 'rb') as f:\n",
    "            self.text_features = pickle.load(f)\n",
    "            \n",
    "        with open(ner_features_file, 'rb') as f:\n",
    "            self.ner_features = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.data_df.iloc[idx]['id']\n",
    "        text = self.data_df.iloc[idx]['cleaned_text']\n",
    "        image_name = self.data_df.iloc[idx]['image']\n",
    "        level_1_target = self.encode(self.data_df.iloc[idx]['Level 1'], 1)\n",
    "        level_2_target = self.encode(self.data_df.iloc[idx]['Level 2'], 2)\n",
    "        level_3_target = self.encode(self.data_df.iloc[idx]['Level 3'], 3)\n",
    "        level_4_target = self.encode(self.data_df.iloc[idx]['Level 4'], 4)\n",
    "        level_5_target = self.encode(self.data_df.iloc[idx]['Level 5'], 5)\n",
    "\n",
    "        # Tokenize text\n",
    "        # encoded_input = tokenizer(text, return_tensors='pt', add_special_tokens=True, \n",
    "        #                           max_length=self.max_len, truncation=True, padding='max_length')\n",
    "        # ids = inputs['input_ids']\n",
    "        # mask = inputs['attention_mask']\n",
    "        # token_type_ids = inputs[\"token_type_ids\"]\n",
    "        image_features = self.image_features[image_name]\n",
    "        text_features = self.text_features[id]\n",
    "        ner_features = self.ner_features[id]\n",
    "\n",
    "        return {\n",
    "            'id': id,\n",
    "            'text': text,\n",
    "            # 'image': image,\n",
    "            'image_features': image_features,  \n",
    "            # 'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            # 'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            # 'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'text_features': text_features,\n",
    "            'ner_features': ner_features,\n",
    "            'level_1_target': level_1_target,\n",
    "            'level_2_target': level_2_target,\n",
    "            'level_3_target': level_3_target,\n",
    "            'level_4_target': level_4_target,\n",
    "            'level_5_target': level_5_target\n",
    "        }\n",
    "\n",
    "    def encode(self, labels, level):\n",
    "        level_ = f'Level {level}'\n",
    "        target = torch.zeros(len(self.labels_at_level[level_]) + 1)\n",
    "        for label in labels:\n",
    "            label_idx = self.labels_at_level[level_][label]\n",
    "            target[label_idx] = 1\n",
    "        if len(labels) == 0:\n",
    "            target[-1] = 1\n",
    "        return target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T10:31:57.072632Z",
     "start_time": "2024-04-27T10:31:57.067007Z"
    }
   },
   "id": "1757e1e47358e1da",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    def __init__(self, df, text_features_file, image_features_file,\n",
    "                 ner_features_file, max_len=128):\n",
    "        super(TestDataSet, self).__init__()\n",
    "        self.data_df = df\n",
    "        \n",
    "        self.image_features = None\n",
    "        self.text_features = None\n",
    "        self.max_len = max_len\n",
    "        # self.tokenizer = tokenizer\n",
    "        with open(image_features_file, 'rb') as f:\n",
    "            self.image_features = pickle.load(f)\n",
    "            \n",
    "        with open(text_features_file, 'rb') as f:\n",
    "            self.text_features = pickle.load(f)\n",
    "            \n",
    "        with open(ner_features_file, 'rb') as f:\n",
    "            self.ner_features = pickle.load(f)\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id = self.data_df.iloc[idx]['id']\n",
    "        text = self.data_df.iloc[idx]['cleaned_text']\n",
    "        image_name = self.data_df.iloc[idx]['image']\n",
    "        # encoded_input = self.tokenizer(text, return_tensors='pt', add_special_tokens=True, \n",
    "        #                           max_length=self.max_len, truncation=True, padding='max_length')\n",
    "        text_features = self.text_features[id]\n",
    "        ner_features = self.ner_features[id]\n",
    "        \n",
    "        return {'id': id,\n",
    "                'text': text,\n",
    "                'text_features': text_features,\n",
    "                'image_features': self.image_features[image_name],\n",
    "                'ner_features': ner_features}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T10:31:57.077836Z",
     "start_time": "2024-04-27T10:31:57.074275Z"
    }
   },
   "id": "dcc6f8ab40dee038",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from modules.nn.MultiModal import MultiModalNER"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:41:31.559923Z",
     "start_time": "2024-04-27T22:41:31.558113Z"
    }
   },
   "id": "32dfb3e0f679e988",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, pred_file_path, gold_file_path, \n",
    "                   evaluator_script_path, id2leaf_label, format=None, validation=False, threshold=0.3):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    HL = HierarchicalLoss(id2label=id2label_subtask_2a, hierarchical_labels=hierarchy_subtask_2a,\n",
    "                        persuasion_techniques=persuasion_techniques_2a, device=device)\n",
    "\n",
    "    total_loss = 0\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            if not isinstance(batch['id'], list):\n",
    "                ids = batch['id'].detach().numpy().tolist()\n",
    "            else:\n",
    "                ids = batch['id']\n",
    "            # encoded_inputs = batch['encoded_input']\n",
    "            # \n",
    "            # input_ids, masks = encoded_inputs['input_ids'], encoded_inputs['attention_mask']\n",
    "            # type_ids = encoded_inputs['token_type_ids']\n",
    "            \n",
    "            \n",
    "            # input_ids = input_ids.squeeze().to(device)\n",
    "            # masks = masks.squeeze().to(device)\n",
    "            # type_ids = type_ids.squeeze().to(device)\n",
    "        \n",
    "            text_features = batch['text_features']\n",
    "            image_features = batch['image_features']\n",
    "            ner_features = batch['ner_features']\n",
    "            \n",
    "            text_features = text_features.to(device)\n",
    "            image_features = image_features.to(device)\n",
    "            ner_features = ner_features.to(device)\n",
    "            \n",
    "            pred_1, pred_2, pred_3, pred_4, pred_5 = model(text_features,image_features, ner_features)\n",
    "            \n",
    "            if validation:\n",
    "                y_1, y_2, y_3 = batch['level_1_target'], batch['level_2_target'], batch['level_3_target']\n",
    "                y_4, y_5 = batch['level_4_target'], batch['level_5_target']\n",
    "                \n",
    "                y_1, y_2, y_3, y_4, y_5 = y_1.to(device), y_2.to(device), y_3.to(device), y_4.to(device), y_5.to(device)\n",
    "                \n",
    "                dloss = HL.calculate_dloss([pred_1, pred_2, pred_3, pred_4, pred_5], [y_1, y_2, y_3, y_4, y_5])\n",
    "                lloss = HL.calculate_lloss([pred_1, pred_2, pred_3, pred_4, pred_5], [y_1, y_2, y_3, y_4, y_5])\n",
    "                \n",
    "                total_loss += (dloss + lloss).detach().cpu().item()\n",
    "                \n",
    "            pred_3 = (pred_3.cpu().detach().numpy() > threshold).astype(int)\n",
    "            pred_4 = (pred_4.cpu().detach().numpy() > threshold).astype(int)\n",
    "            pred_5 = (pred_5.cpu().detach().numpy() > threshold).astype(int)\n",
    "            \n",
    "            predictions += get_labels(id2leaf_label, ids, pred_3, pred_4, pred_5, format)\n",
    "\n",
    "        # Writing JSON data\n",
    "        with open(pred_file_path, 'w') as f:\n",
    "            json.dump(predictions, f, indent=4)\n",
    "        \n",
    "        if gold_file_path is None:\n",
    "            return\n",
    "            \n",
    "        prec_h, rec_h, f1_h = evaluate_h(pred_file_path, gold_file_path)\n",
    "        print(\"f1_h={:.5f}\\tprec_h={:.5f}\\trec_h={:.5f}\".format(f1_h, prec_h, rec_h))\n",
    "        if validation:\n",
    "            return prec_h, rec_h, f1_h, total_loss / (len(dataloader))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:38:20.655823Z",
     "start_time": "2024-04-27T22:38:20.647596Z"
    }
   },
   "id": "adb76f8d9e3a62bd",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = process_json('./semeval2024_dev_release/subtask2a/train.json', \n",
    "                          techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "# val_data = \n",
    "test_data = process_json('./semeval2024_dev_release/subtask2a/validation.json',\n",
    "                         techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "\n",
    "training_dataset = DataSet(train_data, indexed_persuasion_techniques_2a, \n",
    "                           text_features_file='./TextFeatures/subtask2a/text-embedding-3-small/train_text_features.pkl',\n",
    "                           image_features_file='./ImageFeatures/CLIP-ViT/train_images_features.pkl',\n",
    "                           ner_features_file='./TextFeatures/subtask2a/multilingual-ner/train_text_features.pkl')\n",
    "test_dataset = DataSet(test_data, indexed_persuasion_techniques_2a, \n",
    "                       text_features_file='./TextFeatures/subtask2a/text-embedding-3-small/validation_text_features.pkl', \n",
    "                       image_features_file='./ImageFeatures/CLIP-ViT/validation_images_features.pkl',\n",
    "                       ner_features_file='./TextFeatures/subtask2a/multilingual-ner/validation_text_features.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:40:13.525985Z",
     "start_time": "2024-04-27T22:40:12.814139Z"
    }
   },
   "id": "c6480b33f19a6eff",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "\n",
    "device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T21:29:14.739943Z",
     "start_time": "2024-04-27T21:29:14.737177Z"
    }
   },
   "id": "16f63badca9edd10",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nhwhr7rv\n",
      "Sweep URL: https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-mBERT-ResNet/sweeps/nhwhr7rv\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize WandB and log in to your account\n",
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Using Bayesian optimization\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'min': 1e-5,\n",
    "            'max': 1e-4\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [128, 256]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam']\n",
    "        },\n",
    "        'beta1': {  # Relevant for Adam\n",
    "            'min': 0.8,\n",
    "            'max': 0.95\n",
    "        },\n",
    "        # 'momentum': {  # Relevant for SGD\n",
    "        #     'min': 0.8,\n",
    "        #     'max': 0.99\n",
    "        # }\n",
    "        'alpha': {\n",
    "            'min': 0.65,\n",
    "            'max': 1.0\n",
    "        },\n",
    "        'beta': {\n",
    "            'min': 0.5,\n",
    "            'max': 1.0\n",
    "        },\n",
    "        'threshold':{\n",
    "            'min': 0.65,\n",
    "            'max': 0.9\n",
    "            }\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"FineTuning-MultiModal-OpenAI-Small-NER\")\n",
    "# sweep_id = '44uz6ydx'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:40:59.786367Z",
     "start_time": "2024-04-27T22:40:59.275666Z"
    }
   },
   "id": "b44a75926da4f6e3",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "models_dir = './models/subtask2a/MultiModal-OpenAI-Small-NER/'\n",
    "num_epochs = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:41:02.308901Z",
     "start_time": "2024-04-27T22:41:02.306206Z"
    }
   },
   "id": "9f7f2d6769c8d7bf",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train():\n",
    "    wandb.init()\n",
    "\n",
    "    # Use WandB configurations\n",
    "    config = wandb.config\n",
    "    batch_size = config.batch_size\n",
    "    learning_rate = config.learning_rate\n",
    "    \n",
    "    train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = MultiModalNER(512, 1536, 768)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = None\n",
    "    \n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(config.beta1, 0.999)\n",
    "        )\n",
    "    elif config.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            momentum=config.momentum\n",
    "        )\n",
    "        \n",
    "    HL = HierarchicalLoss(id2label=id2label_subtask_2a, hierarchical_labels=hierarchy_subtask_2a,\n",
    "                        persuasion_techniques=persuasion_techniques_2a, device=device, \n",
    "                          alpha=config.alpha, beta=config.beta, threshold=config.threshold)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            y_1, y_2, y_3 = batch['level_1_target'], batch['level_2_target'], batch['level_3_target']\n",
    "            y_4, y_5 = batch['level_4_target'], batch['level_5_target']\n",
    "            \n",
    "            # encoded_inputs = batch['encoded_input']\n",
    "            images_features = batch['image_features']\n",
    "            text_features = batch['text_features']\n",
    "            ner_features = batch['ner_features'] \n",
    "                \n",
    "            # input_ids, masks = encoded_inputs['input_ids'], encoded_inputs['attention_mask']\n",
    "            # type_ids = encoded_inputs['token_type_ids']\n",
    "            \n",
    "            # input_ids = input_ids.squeeze().to(device)\n",
    "            # masks = masks.squeeze().to(device)\n",
    "            # type_ids = type_ids.squeeze().to(device)\n",
    "            \n",
    "            y_1, y_2, y_3, y_4, y_5 = y_1.to(device), y_2.to(device), y_3.to(device), y_4.to(device), y_5.to(device)\n",
    "            \n",
    "            images_features = images_features.to(device)\n",
    "            text_features = text_features.to(device)\n",
    "            ner_features = ner_features.to(device)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_1, pred_2, pred_3, pred_4, pred_5 = model(text_features, images_features, ner_features)\n",
    "            # loss_ = loss(pred_1, y_1) + loss(pred_2, y_2) + loss(pred_3, y_3) + loss(pred_4, y_4) + loss(pred_5, y_5)\n",
    "            \n",
    "            dloss = HL.calculate_dloss([pred_1, pred_2, pred_3, pred_4, pred_5], [y_1, y_2, y_3, y_4, y_5])\n",
    "            lloss = HL.calculate_lloss([pred_1, pred_2, pred_3, pred_4, pred_5], [y_1, y_2, y_3, y_4, y_5])\n",
    "    \n",
    "            total_loss = lloss + dloss\n",
    "            # loss_.backward()\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += total_loss.detach().item()\n",
    "            \n",
    "            # if batch_idx % 20 == 19:\n",
    "            #     print(f\"[{epoch + 1}, {batch_idx + 1}] loss: {running_loss / 20:.3f}\")\n",
    "            #     running_loss = 0.0\n",
    "                \n",
    "        running_loss /= len(train_dataloader)\n",
    "        \n",
    "        val_pred_file_path = './Predictions/val_predictions_subtask2a.json'\n",
    "        val_gold_file_path = './semeval2024_dev_release/subtask2a/validation.json'\n",
    "        evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "        prec_h, rec_h, f1_h, validation_loss = evaluate_model(model, test_dataloader, val_pred_file_path, val_gold_file_path, evaluator_script,\n",
    "                       id2leaf_label_subtask_2a, validation=True)\n",
    "        \n",
    "        if epoch % 50 == 49:\n",
    "            print(f'[{epoch+1}/{num_epochs}]')\n",
    "            print(\"f1_h={:.5f}\\tprec_h={:.5f}\\trec_h={:.5f}\".format(f1_h, prec_h, rec_h))\n",
    "        \n",
    "        # Log training metrics\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": running_loss})\n",
    "        wandb.log({\"val_loss\": validation_loss})\n",
    "        wandb.log({\"h_precision\": prec_h, \"h_recall\": rec_h, \"h_f1-score\":f1_h})\n",
    "        \n",
    "    \n",
    "    torch.save(model.state_dict(), f\"{models_dir}{wandb.run.name}.pth\")\n",
    "    wandb.join()\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T21:32:37.506507Z",
     "start_time": "2024-04-27T21:32:37.498652Z"
    }
   },
   "id": "f23511341846e972",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: h8gfcbnt with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \talpha: 0.8148407006372308\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 128\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta: 0.9304946887751062\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta1: 0.8548119477478157\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 3.783507948650642e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \toptimizer: adam\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tthreshold: 0.8803527984779607\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/iqbal/Desktop/Python Assignments/Detecting-Persuasion-Techniques-in-Memes/wandb/run-20240427_173241-h8gfcbnt</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/h8gfcbnt' target=\"_blank\">ruby-sweep-1</a></strong> to <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View sweep at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/h8gfcbnt' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/h8gfcbnt</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/100]\n",
      "f1_h=0.64432\tprec_h=0.65570\trec_h=0.63332\n",
      "[100/100]\n",
      "f1_h=0.65329\tprec_h=0.66307\trec_h=0.64380\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9eb60d244ad4cbd97898058c57d5660"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>h_f1-score</td><td>▁▁▃▄▅▆▇▇▇▇▇▇▇▇▇▇█▇▇▇██████████████▇▇▇███</td></tr><tr><td>h_precision</td><td>▁▄▅▅█▇▆▅▆▆▃▅▄▄▄▅▂▅▆▄▄▅▆▅▆▆▅▅▄▅▅▅▄▄▆▅▄▅▄▄</td></tr><tr><td>h_recall</td><td>▁▁▃▄▄▅▆▆▆▆▇▆▇▇▇▇█▇▇▇█▇▇▇▇▇███▇▇███▆▇███▇</td></tr><tr><td>train_loss</td><td>██▇▇▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>▆█▇▄▅▂▃▅▃▄▃▃▃▄▄▂▂▂▁▂▃▂▂▃▂▃▂▃▄▂▃▄▄▂▄▄▅▂▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>h_f1-score</td><td>0.65329</td></tr><tr><td>h_precision</td><td>0.66307</td></tr><tr><td>h_recall</td><td>0.6438</td></tr><tr><td>train_loss</td><td>520.90003</td></tr><tr><td>val_loss</td><td>870.31638</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">ruby-sweep-1</strong> at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/h8gfcbnt' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/h8gfcbnt</a><br/> View project at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240427_173241-h8gfcbnt/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: lf622zry with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \talpha: 0.6773769641445323\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 128\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta: 0.8801147810553063\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta1: 0.8063826091240119\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 9.71121854299423e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \toptimizer: adam\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tthreshold: 0.728081155498142\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/iqbal/Desktop/Python Assignments/Detecting-Persuasion-Techniques-in-Memes/wandb/run-20240427_174638-lf622zry</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/lf622zry' target=\"_blank\">winter-sweep-2</a></strong> to <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View sweep at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/lf622zry' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/lf622zry</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/100]\n",
      "f1_h=0.66207\tprec_h=0.64359\trec_h=0.68165\n",
      "[100/100]\n",
      "f1_h=0.65206\tprec_h=0.65562\trec_h=0.64853\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "875f931483f94dbb8537d6b57b79a1fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>h_f1-score</td><td>▁▄▆▇▇▇▇▇▇██▇██████████▇██▇████▇█████████</td></tr><tr><td>h_precision</td><td>▄▁█▁▅▅▅▄▃▃▃▆▄▃▂▂▄▄▂▂▃▃▁▄▃▂▃▄▃▄▂▃▃▃▄▃▃▄▃▃</td></tr><tr><td>h_recall</td><td>▁▄▄▇▆▆▆▆▇▇▇▆▇███▇▇█▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss</td><td>██▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▂▃▃▁▁▂▁▂▁▂▁▂▃▂▄▄▂▂▃▃▃▃▄▅▄▄▅▅▅▄▃▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>h_f1-score</td><td>0.65206</td></tr><tr><td>h_precision</td><td>0.65562</td></tr><tr><td>h_recall</td><td>0.64853</td></tr><tr><td>train_loss</td><td>102.73471</td></tr><tr><td>val_loss</td><td>919.15292</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">winter-sweep-2</strong> at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/lf622zry' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/lf622zry</a><br/> View project at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240427_174638-lf622zry/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: brkaqdpf with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \talpha: 0.8747963915862391\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 128\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta: 0.813983245207873\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta1: 0.8163857352344261\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 1.3429958239991307e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \toptimizer: adam\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tthreshold: 0.6858302547729106\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/iqbal/Desktop/Python Assignments/Detecting-Persuasion-Techniques-in-Memes/wandb/run-20240427_180111-brkaqdpf</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/brkaqdpf' target=\"_blank\">balmy-sweep-3</a></strong> to <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View sweep at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/brkaqdpf' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/brkaqdpf</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/100]\n",
      "f1_h=0.63326\tprec_h=0.66518\trec_h=0.60426\n",
      "[100/100]\n",
      "f1_h=0.65234\tprec_h=0.65344\trec_h=0.65123\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f4cbe7c42af43ffaa97dc2779cfc276"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>h_f1-score</td><td>▁▃▃▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█████████████</td></tr><tr><td>h_precision</td><td>▁██▇▇▇███████████████████▇█▇███▇▇█▇█▇▇▇▇</td></tr><tr><td>h_recall</td><td>█▁▁▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>train_loss</td><td>█▆▆▆▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▅▅▅██▆▅▄▃▂▃▃▃▃▁▃▃▂▂▂▂▃▃▂▃▃▃▃▅▃▄▃▃▂▃▁▃▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>h_f1-score</td><td>0.65234</td></tr><tr><td>h_precision</td><td>0.65344</td></tr><tr><td>h_recall</td><td>0.65123</td></tr><tr><td>train_loss</td><td>562.80384</td></tr><tr><td>val_loss</td><td>830.03854</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">balmy-sweep-3</strong> at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/brkaqdpf' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/brkaqdpf</a><br/> View project at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240427_180111-brkaqdpf/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: yadyyyre with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \talpha: 0.8613357510926161\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 256\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta: 0.7292018368808355\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta1: 0.8675434001226184\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 3.241287170425779e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \toptimizer: adam\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tthreshold: 0.701974944206003\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/iqbal/Desktop/Python Assignments/Detecting-Persuasion-Techniques-in-Memes/wandb/run-20240427_181513-yadyyyre</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/yadyyyre' target=\"_blank\">effortless-sweep-4</a></strong> to <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View sweep at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/yadyyyre' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/yadyyyre</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/100]\n",
      "f1_h=0.63622\tprec_h=0.64655\trec_h=0.62623\n",
      "[100/100]\n",
      "f1_h=0.65119\tprec_h=0.65318\trec_h=0.64921\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13551a23e88740b5b8c898755f232c13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>h_f1-score</td><td>▁▁▃▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█████▇████▇█████▇████</td></tr><tr><td>h_precision</td><td>▁█▇▇▇▇█████▇██▇████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>h_recall</td><td>█▁▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▆▅▆▅▆▅▅▆▆▆▅▆▅▆▅▆▆</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▅█▇▇▆▄▃▂▂▁▂▂▂▂▄▂▂▃▂▃▃▁▄▂▃▁▂▂▂▂▂▂▁▂▄▃▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>h_f1-score</td><td>0.65119</td></tr><tr><td>h_precision</td><td>0.65318</td></tr><tr><td>h_recall</td><td>0.64921</td></tr><tr><td>train_loss</td><td>845.89249</td></tr><tr><td>val_loss</td><td>1701.55682</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">effortless-sweep-4</strong> at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/yadyyyre' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/yadyyyre</a><br/> View project at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240427_181513-yadyyyre/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: ge3y1c7j with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \talpha: 0.8104726890924978\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 256\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta: 0.9962148653392854\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbeta1: 0.9464458722543008\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlearning_rate: 1.6605121085307106e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \toptimizer: adam\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tthreshold: 0.7736966038651765\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.6"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/iqbal/Desktop/Python Assignments/Detecting-Persuasion-Techniques-in-Memes/wandb/run-20240427_182532-ge3y1c7j</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/ge3y1c7j' target=\"_blank\">electric-sweep-5</a></strong> to <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View sweep at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/sweeps/rysbmph7</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/ge3y1c7j' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/ge3y1c7j</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/100]\n",
      "f1_h=0.62527\tprec_h=0.68253\trec_h=0.57688\n",
      "[100/100]\n",
      "f1_h=0.63892\tprec_h=0.65325\trec_h=0.62521\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "063f2d9aae3f4633b77a0de889128098"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>h_f1-score</td><td>▁▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>h_precision</td><td>▁██████▇▇▇▇▇███████████████████████████▇</td></tr><tr><td>h_recall</td><td>█▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▃▄▃▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>train_loss</td><td>█▆▇▇▆▆▆▆▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▂▂▃▃▃▃▃▃▃▂▂▁▁▂▁▂▁▁▁▁▁▂▁▂▁▁▁▁▂▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>h_f1-score</td><td>0.63892</td></tr><tr><td>h_precision</td><td>0.65325</td></tr><tr><td>h_recall</td><td>0.62521</td></tr><tr><td>train_loss</td><td>1565.28675</td></tr><tr><td>val_loss</td><td>1680.8139</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">electric-sweep-5</strong> at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/ge3y1c7j' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER/runs/ge3y1c7j</a><br/> View project at: <a href='https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER' target=\"_blank\">https://wandb.ai/phoenix_nlp/FineTuning-MultiModal-OpenAI-Small-NER</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240427_182532-ge3y1c7j/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    wandb.agent(sweep_id, train, count=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:37:04.275083Z",
     "start_time": "2024-04-27T21:32:39.391489Z"
    }
   },
   "id": "9de3bd4e79725578",
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "\n",
    "#### OpenAI Large + CLIP + NER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bc653f7786d2e26"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiModalNER(512, 3072, 768)\n",
    "model.load_state_dict(torch.load(f\"./models/subtask2a/MultiModal-OpenAI-Large-NER/royal-sweep-3.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T21:26:54.229098Z",
     "start_time": "2024-04-27T21:26:53.645054Z"
    }
   },
   "id": "390cc97574e1aa7f",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.54513\tprec_h=0.58333\trec_h=0.51163\n"
     ]
    }
   ],
   "source": [
    "ar_pred_file_path = './Predictions/subtask2a/ar_predictions_subtask2a.txt'\n",
    "ar_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_ar.json'\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "ar_test_data = process_json(ar_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "ar_test_dataset = TestDataSet(df=ar_test_data, \n",
    "                          text_features_file='./TextFeatures/subtask2a/text-embedding-3-large/ar_test_text_features.pkl',\n",
    "                          image_features_file='./ImageFeatures/CLIP-ViT/ar_test_images_features.pkl', \n",
    "                        ner_features_file='./TextFeatures/subtask2a/multilingual-ner/ar_test_text_features.pkl')\n",
    "\n",
    "ar_test_dataloader =  DataLoader(ar_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_model(model, ar_test_dataloader, ar_pred_file_path, ar_gold_file_path, evaluator_script, \n",
    "               id2leaf_label_subtask_2a, format=5, validation=False, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T21:27:46.740081Z",
     "start_time": "2024-04-27T21:27:46.653949Z"
    }
   },
   "id": "cecd2f8fd335f629",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.63279\tprec_h=0.66017\trec_h=0.60759\n"
     ]
    }
   ],
   "source": [
    "bg_pred_file_path = './Predictions/subtask2a/bg_predictions_subtask2a.txt'\n",
    "bg_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_bg.json'\n",
    "\n",
    "bg_test_data = process_json(bg_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "bg_test_dataset = DataSet(bg_test_data, indexed_persuasion_techniques_2a, \n",
    "                           text_features_file='./TextFeatures/subtask2a/text-embedding-3-large/bg_test_text_features.pkl',\n",
    "                          image_features_file='./ImageFeatures/CLIP-ViT/bulgarian_test_images_features.pkl',\n",
    "                          ner_features_file='./TextFeatures/subtask2a/multilingual-ner/bg_test_text_features.pkl')\n",
    "\n",
    "bg_test_dataloader = DataLoader(bg_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_model(model, bg_test_dataloader, bg_pred_file_path, bg_gold_file_path, evaluator_script, \n",
    "               id2leaf_label_subtask_2a, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T21:27:56.785373Z",
     "start_time": "2024-04-27T21:27:56.497812Z"
    }
   },
   "id": "f8e9e3a800d3d09d",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.67973\tprec_h=0.75972\trec_h=0.61499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "md_pred_file_path = './Predictions/subtask2a/md_predictions_subtask2a.txt'\n",
    "md_gold_file_path = 'test_labels_ar_bg_md_version2/test_subtask2a_md.json'\n",
    "\n",
    "md_test_data = process_json(md_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "md_test_dataset = DataSet(md_test_data, indexed_persuasion_techniques_2a, \n",
    "                          text_features_file='./TextFeatures/subtask2a/text-embedding-3-large/md_test_text_features.pkl',\n",
    "                          image_features_file='./ImageFeatures/CLIP-ViT/nm_test_images_features.pkl',\n",
    "                          ner_features_file='./TextFeatures/subtask2a/multilingual-ner/md_test_text_features.pkl')\n",
    "\n",
    "md_test_dataloader = DataLoader(md_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_model(model, md_test_dataloader, md_pred_file_path, md_gold_file_path, evaluator_script, \n",
    "               id2leaf_label_subtask_2a, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T21:28:03.473984Z",
     "start_time": "2024-04-27T21:28:03.272812Z"
    }
   },
   "id": "7063c2173a79137d",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "en_pred_file_path = './Predictions/subtask2a/en_predictions_subtask2a.txt'\n",
    "\n",
    "eng_test_data = process_test_json('./test_data/english/en_subtask2a_test_unlabeled.json')\n",
    "\n",
    "eng_test_dataset = TestDataSet(eng_test_data, \n",
    "                               './TextFeatures/subtask2a/text-embedding-3-large/en_test_text_features.pkl', \n",
    "                               './ImageFeatures/CLIP-ViT/english_test_images_features.pkl',\n",
    "                               './TextFeatures/subtask2a/multilingual-ner/en_test_text_features.pkl')\n",
    "\n",
    "eng_test_dataloader = DataLoader(eng_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_model(model, eng_test_dataloader, en_pred_file_path, None, evaluator_script, \n",
    "               id2leaf_label_subtask_2a, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T21:28:15.371930Z",
     "start_time": "2024-04-27T21:28:14.597829Z"
    }
   },
   "id": "1553934ec625090e",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "0.68315\t0.70546\t0.66221"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9759b5791ade4105"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### OpenAI Small + CLIP + NER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1241365d2df8bf19"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiModalNER(512, 1536, 768)\n",
    "model.load_state_dict(torch.load(f\"./models/subtask2a/MultiModal-OpenAI-Small-NER/balmy-sweep-3.pth\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:38:03.898643Z",
     "start_time": "2024-04-27T22:38:03.530291Z"
    }
   },
   "id": "e56c1185ea29fc13",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.52085\tprec_h=0.53403\trec_h=0.50831\n"
     ]
    }
   ],
   "source": [
    "ar_pred_file_path = './Predictions/subtask2a/ar_predictions_subtask2a.txt'\n",
    "ar_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_ar.json'\n",
    "evaluator_script = './scorer-baseline/subtask_1_2a.py'\n",
    "\n",
    "ar_test_data = process_json(ar_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "ar_test_dataset = TestDataSet(df=ar_test_data, \n",
    "                          text_features_file='./TextFeatures/subtask2a/text-embedding-3-small/ar_test_text_features.pkl',\n",
    "                          image_features_file='./ImageFeatures/CLIP-ViT/ar_test_images_features.pkl', \n",
    "                        ner_features_file='./TextFeatures/subtask2a/multilingual-ner/ar_test_text_features.pkl')\n",
    "\n",
    "ar_test_dataloader =  DataLoader(ar_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_model(model, ar_test_dataloader, ar_pred_file_path, ar_gold_file_path, evaluator_script, \n",
    "               id2leaf_label_subtask_2a, format=5, validation=False, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:38:34.409649Z",
     "start_time": "2024-04-27T22:38:34.333840Z"
    }
   },
   "id": "d722fa362f25802e",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.62882\tprec_h=0.67538\trec_h=0.58828\n"
     ]
    }
   ],
   "source": [
    "bg_pred_file_path = './Predictions/subtask2a/bg_predictions_subtask2a.txt'\n",
    "bg_gold_file_path = './test_labels_ar_bg_md_version2/test_subtask2a_bg.json'\n",
    "\n",
    "bg_test_data = process_json(bg_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "bg_test_dataset = DataSet(bg_test_data, indexed_persuasion_techniques_2a, \n",
    "                           text_features_file='./TextFeatures/subtask2a/text-embedding-3-small/bg_test_text_features.pkl',\n",
    "                          image_features_file='./ImageFeatures/CLIP-ViT/bulgarian_test_images_features.pkl',\n",
    "                          ner_features_file='./TextFeatures/subtask2a/multilingual-ner/bg_test_text_features.pkl')\n",
    "\n",
    "bg_test_dataloader = DataLoader(bg_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_model(model, bg_test_dataloader, bg_pred_file_path, bg_gold_file_path, evaluator_script, \n",
    "               id2leaf_label_subtask_2a, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:38:37.627586Z",
     "start_time": "2024-04-27T22:38:37.384567Z"
    }
   },
   "id": "b52c4f61cb10b95b",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_h=0.66341\tprec_h=0.76865\trec_h=0.58352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "md_pred_file_path = './Predictions/subtask2a/md_predictions_subtask2a.txt'\n",
    "md_gold_file_path = 'test_labels_ar_bg_md_version2/test_subtask2a_md.json'\n",
    "\n",
    "md_test_data = process_json(md_gold_file_path, techniques_to_level_2a, hierarchy_subtask_2a)\n",
    "md_test_dataset = DataSet(md_test_data, indexed_persuasion_techniques_2a, \n",
    "                          text_features_file='./TextFeatures/subtask2a/text-embedding-3-small/md_test_text_features.pkl',\n",
    "                          image_features_file='./ImageFeatures/CLIP-ViT/nm_test_images_features.pkl',\n",
    "                          ner_features_file='./TextFeatures/subtask2a/multilingual-ner/md_test_text_features.pkl')\n",
    "\n",
    "md_test_dataloader = DataLoader(md_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_model(model, md_test_dataloader, md_pred_file_path, md_gold_file_path, evaluator_script, \n",
    "               id2leaf_label_subtask_2a, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:38:40.715392Z",
     "start_time": "2024-04-27T22:38:40.544172Z"
    }
   },
   "id": "ae89e600f6707a95",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "en_pred_file_path = './Predictions/subtask2a/en_predictions_subtask2a.txt'\n",
    "\n",
    "eng_test_data = process_test_json('./test_data/english/en_subtask2a_test_unlabeled.json')\n",
    "\n",
    "eng_test_dataset = TestDataSet(eng_test_data, \n",
    "                               './TextFeatures/subtask2a/text-embedding-3-small/en_test_text_features.pkl', \n",
    "                               './ImageFeatures/CLIP-ViT/english_test_images_features.pkl',\n",
    "                               './TextFeatures/subtask2a/multilingual-ner/en_test_text_features.pkl')\n",
    "\n",
    "eng_test_dataloader = DataLoader(eng_test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "evaluate_model(model, eng_test_dataloader, en_pred_file_path, None, evaluator_script, \n",
    "               id2leaf_label_subtask_2a, threshold=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-27T22:38:45.861656Z",
     "start_time": "2024-04-27T22:38:45.198399Z"
    }
   },
   "id": "ce9670a7544dc865",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "0.67304\t0.70024\t0.64787"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5b5621380ab2464"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
